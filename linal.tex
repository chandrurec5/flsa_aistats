%!TEX root =  flsa.tex
\appendix

\section{Proofs for \cref{sec:mainresults}}
\subsection{Hurwitz matrices and positive definiteness}
\label{sec:hurpd}
The symmetric part of a real-valued matrix $A$ is $(A+A^\top)/2$.
The following lemma states that Hurwitz matrices are similar to some matrix whose symmetric part is positive definite.
After the lemma we give an example that shows that the similarity cannot be chosen to be the identity matrix.
\begin{lemma}\label{lm:simtran}
Any Hurwitz matrix is similar to a real matrix whose symmetric part is positive definite.
Furthermore, the similarity transformation can be chosen to be an SPD matrix. \todoc{The reverse is also true, right..?}
\end{lemma}
\begin{proof}
Recall that if $A\in \R^{d\times d}$ is Hurwitz, then there exist a unique SPD matrix $P$ such that the Lyapunov equation
\begin{align*}
A^\top P + P A = I
\end{align*}
is satisfied (e.g., Lemma A.23 of \citet{french2003}).
Take $U = P^{1/2}$. Clearly, $U$ is positive definite and symmetric.
Let $B = U^{-1} A U$. 
Twice the symmetric part of $B$ is 
\begin{align*}
B+B^\top 
 = P^{-1/2} A P^{1/2} + P^{1/2} A^\top P^{-1/2} 
 = P^{-1/2} ( A P +  P A^\top ) P^{-1/2} = P^{-1}\,,
\end{align*}
which is positive definite, hence finishing the proof.
\end{proof}
Now, consider the matrix 
\begin{align*}
A = 
\begin{pmatrix}
1 & -10 \\
0 & 1
\end{pmatrix}\,.
\end{align*}
We claim that this is a Hurwitz matrix, but $A+A^\top$ is not positive definite.
Indeed, $A$ has a single eigenvalue of multiplicity two, which is equal to one. Thus, $A$ is Hurwitz.
On the other hand,
\begin{align*}
A + A^\top = 
\begin{pmatrix}
2 & -10 \\
-10 & 2
\end{pmatrix}\,.
\end{align*}
The eigenvalues of this matrix solve $(2-\lambda)^2 - 100=0$, i.e., they are
$\lambda_1 = 12$ and $\lambda_2 = -8$. Hence, $A+A^\top$ is indeed not SPD.

We note in passing that the change of coordinates argument that we follow here is inspired 
by the paper of \citet{lihong}. 

\subsection{Error bound when $\rhos{P_A}\ge 0$}
In this section we show a bound on the MSE, $\E[\norm{\eh_t}^2]$, for the case when $\rhos{P_A}\ge 0$.
Following the suggestion in the main body of the paper, the general case will be reduced to this case by using 
\cref{lm:simtran}.
The relations presented below that connect expressions that involve conditional expectations hold almost surely (a.s.).
However, to minimize clutter, the ``a.s.'' qualifier will be omitted from their statement.

Let $b = b_P$.
Recall that 
\begin{align*}
e_t =  \theta_t-\ts\,, \qquad 
\eh_t = \thh_t-\ts\,, \qquad  \text{and} \qquad 
\zeta_t = b_t-b-(A_t-A)\ts\,.
\end{align*}
Further, introduce
\begin{align*}
F_{i,j} & =
\begin{cases}
 (I-\alpha A_i)(I-\alpha A_{i-1}) \dots (I-\alpha A_j)\,, & \text{if } i\ge j\,;\\
 I\,, & \text{otherwise}\,.
\end{cases}
\end{align*}
Let $\sigma_1^2$ be so that 
%and define $\sigma_1^2 \eqdef 2( \sigma_{A_P}^2\norm{\ts}^2+\sigma_{b_P}^2)$ so that 
$\EE{\normsm{\zeta_t}^2}\leq \sigma_1^2$.
Note that we can always choose $\sigma_1^2 = 2( \sigma_{A_P}^2\norm{\ts}^2+\sigma_{b_P}^2)$.

\paragraph{Error Recursion}
Recall that 
$\theta_t=\theta_{t-1}+\alpha\big(b_t-A_t\theta_{t-1}\big)$.
Subtracting $\ts$ from both sides, we get
\begin{align*}
\theta_t-\ts&=\theta_{t-1}-\ts+\alpha\big(b_t-A_t(\theta_{t-1}-\ts+\ts)\big)\,,
\end{align*}
and substituting $e_t$ and using $A\ts-b=0$ we get
\begin{align*}
e_t 
	&= (I-\alpha A_t) e_{t-1} + \alpha (b_t - A_t \ts ) 
	   = (I-\alpha A_t) e_{t-1} + \alpha (b_t-b - (A_t-A) \ts ) \\
	 &= (I-\alpha A_t) e_{t-1} + \alpha \zeta_t\,.
	   \label{eq:errrec} \numberthis
\end{align*}
Unrolling this for $t-1,t-2,\dots,1$, we get
\begin{align}
e_t
%&={\prod_{s=1}^t \! (I-\alpha A_s) e_0}+\alpha{\sum_{i=1}^t\! \left\{\prod_{s=i+1}^t \!(I-\alpha A_s)\!\right\}\!\zeta_i}\\
&=F_{t,1} e_0+\alpha\, \sum_{i=1}^t F_{t,i+1} \zeta_i\,,
 \label{eq:etft}
\end{align}
which matches \eqref{eq:errrecmain}.
Now, $\eh_t = \frac{1}{t+1} \sum_{i=0}^t e_i$ and hence
\begin{align}
\EE{\normsm{\eh_t}^2} = \frac{1}{(t+1)^2} \sum_{i,j=0}^t \EE{ \ip{e_i,e_j} }\,.
\label{eq:ehtexp}
\end{align}
To bound the terms $\EE{ \ip{e_i,e_j} }$ the following lemma that 
uses the martingale structure of the noise will be useful:
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma}
  \label{lem:mart}
  The following hold:
  \begin{enumerate}[label=(\emph{\roman*})]
  %%%%%%%%%%%%%%%%%%%%%%%%%%%
  \item \label{lem:mart:genunroll}
    Let $1\le i<t$ and let $x,y\in \R^d$ be $\F_{i}$-measurable random vectors. Then,
    \begin{align*}
    \E[x^\top F_{t,i+1}y|\F_i]=x^\top (I-\alpha A)^{t-i} y\,.
    \end{align*}
  %%%%%%%%%%%%%%%%%%%%%%%%%%%
  \item \label{lem:mart:noisecancel}
    Let $1\le i <t$  and let $x\in \R^d$ be an $\F_{i-1}$-measurable random vector. Then,
    $\E[x^\top F_{t,i+1}\zeta_{i}]=0$.
    \item \label{lem:mart:unroll}
    For all $0\le i < t$, $\E[ \ip{e_i,F_{t,i+1} e_i}]=\E[\ip{e_i,(I-\alpha A)^{t-i} e_i}]$.
  %%%%%%%%%%%%%%%%%%%%%%%%%%%
  \item \label{lem:mart:innerproduct}
    Let $0\le j <i$ and let $x\in \R^d$ be an $\F_j$-measurable random vector.
    Then,
    \begin{align*}
    \E[\ip{F_{i,j+1}x,F_{i,j+1}x}]\leq (1-\alpha \rhos{P_A})^{i-j}\,\E\norm{x}^2\,.
    \end{align*}
  \end{enumerate}
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We start by noting that for arbitrary $t,i$, $F_{t,i}$ is $\cF_t$-measurable. Indeed, this holds
because $F_{t,i}$ is a function of $\{A_s\}_{\min(t,i)\le s \le t}$ only.

\noindent \textbf{Part \ref{lem:mart:genunroll}:}
By definition $F_{t,i+1} = (I-\alpha A_t) F_{t-1,i+1}$.
By our note above, $F_{t-1,i+1}$ is $\F_{t-1}$-measurable.
Since $x$ and $y$ are also $\F_{t-1}$-measurable, we get 
\begin{align*}
\EE{x^\top F_{t,i+1} y | \F_{t-1} } &= x^\top \EE{ (I-\alpha A_t) | \F_{t-1} } F_{t-1,i+1} y=x^\top  (I-\alpha A)  F_{t-1,i+1} y\,.
\end{align*}
If $t-1=i$, the proof is done. When $t-1\ge i+1$,
by the tower-rule for conditional expectations and our measurability assumptions,
\begin{align*}
\EE{x^\top F_{t,i+1} y | \F_{t-2} }=x^\top  (I-\alpha A)  \EE{F_{t-1,i+1} |\F_{t-2}} y= x^\top (I-\alpha A)^2 F_{t-2,i+1} y\,.
\end{align*}
Continuing this way we get
\begin{align*}
\EE{x^\top F_{t,i+1} y | \F_{t-j} }
= x^\top (I-\alpha A)^j F_{t-j,i+1} y\,, \quad j=1,2,\dots,t-i\,.
\end{align*}
Specifically, for $j=t-i$ we get
\begin{align*}
\EE{x^\top F_{t,i+1} y | \F_{i} }  = x^\top (I-\alpha A)^{t-i} y\,.
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent \textbf{Part \ref{lem:mart:noisecancel}:}
By part \ref{lem:mart:genunroll},
\begin{align*}
\EE{x^\top F_{t,i+1} \zeta_i | \F_{i} }  = x^\top (I-\alpha A)^{t-i} \zeta_i\,.
\end{align*}
Using the tower rule,
\begin{align*}
\EE{x^\top F_{t,i+1} \zeta_i | \F_{i-1} }
= x^\top (I-\alpha A)^{t-i}\EE{ \zeta_i | \F_{i-1} }= 0\,.
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent \textbf{Part \ref{lem:mart:unroll}}:
The result follows directly from part~\ref{lem:mart:genunroll}. Indeed,
$\theta_i$ depends only on $A_1,\dots,A_{i},b_1,\dots,b_{i}$ and so  $e_i$ is $\F_i$-measurable.
Hence, by part~\ref{lem:mart:genunroll},
\begin{align*}
\EE{ \ip{e_i, F_{t,i+1} e_i} | \F_i } =
\EE{ \ip{e_i, (I-\alpha A)^{t-i} e_i} | \F_i }\,.
\end{align*}
Taking expectation of both sides gives the desired result.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent \textbf{Part \ref{lem:mart:innerproduct}}:
Note that
$S_t\doteq \EE{ (I-\alpha A_t)^\top (I-\alpha A_t) | \F_{t-1} }
= I - \alpha (A^\top + A) + \alpha^2 \EE{ A_t^\top A_t | \F_{t-1} }$.
Since $\{A_t\}_{t\ge 1}$ is an i.i.d. sequence, $\EE{ A_t^\top A_t|\F_{t-1}} = \EE{ A_1^\top A_1 }$.
Now, using the definition of $\rhos{P_A}$ (cf. \cref{def:spect}),
$\sup_{x:\norm{x}= 1} x^\top S_t x = 1 - \alpha \inf_{x:\norm{x}=1} x^\top (A^\top + A - \alpha \EE{A_1^\top A_1}) x
= 1-\alpha \rhos{P_A}$.
Hence,
\begin{align*}
\EE{\ip{F_{i,j+1}x,F_{i,j+1}x}|\F_{i-1} }
&= \EE{x^\top F_{i-1,j+1}^\top (I-\alpha A_i)^\top (I-\alpha A_i) F_{i-1,j+1} x\,|\,\F_{i-1}}\\
&=(x F_{i-1,j+1})^\top \, S_i \, F_{i-1,j+1} x\\
&\le (1-\alpha \rhos{P_A}) \, \ip{ F_{i-1,j+1} x, F_{i-1,j+1} x}
\end{align*}
When $i=j-1$, $F_{i-1,j+1}=I$ and the proof is done by taking expectations of both sides.
Otherwise, $i>j-1$ and repeating the same calculation as above
and using the tower rule for conditional expectations we get
\begin{align*}
\EE{\ip{F_{i,j+1}x,F_{i,j+1}x}|\F_{i-2}}
&=
\EE{\EE{\ip{F_{i,j+1}x,F_{i,j+1}x}|\F_{i-1} }|\F_{i-2}}\\
& \le (1-\alpha \rhos{P_A}) \, \EE{\ip{ F_{i-1,j+1} x, F_{i-1,j+1} x}|\F_{i-2}} \\
& \le (1-\alpha \rhos{P_A})^2\, \ip{ F_{i-2,j+1} x, F_{i-2,j+1} x}\,.
\end{align*}
If $i=j-2$, $F_{i-2,j+1}=I$ and taking expectations of both sides the proof is finished.
Continuing the same way we get
\begin{align*}
\EE{\ip{F_{i,j+1}x,F_{i,j+1}x}|\F_{j}}
%&= \EE{\EE{ \ip{F_{i,j+1}x,F_{i,j+1}x} |\F_{j+1}}|\F_{j}} 
%& \le (1-\alpha \rhos{P_A})^{j-i-1} \ip{ F_{i+1-j,j+1} x, F_{i+1-j,j+1} x}
& \le (1-\alpha \rhos{P_A})^{j-i} \norm{x}^2\,.
\end{align*}
Taking expectations of both sides finishes the proof.
\end{proof}

Returning to bounding $\E[ \normsm{\eh_t}^2 ]$ based on \eqref{eq:ehtexp}.
We now show that the contribution of the ``cross-terms'', $\sum_{i=0}^t \sum_{j=i+1}^t \EE{ \ip{e_i,e_j} }$, 
on the right-hand side of \eqref{eq:ehtexp}
is proportional to the contribution of the ``diagonal terms'', $\sum_{i=0}^t \EE{ \ip{e_i,e_i} }$.
For the bound, recall that $\rhod{P_A} = \lmin{ A + A^\top - \alpha A^\top A }$.
\begin{lemma}
\label{lem:crosstermbound}
Assume that $\rhod{P_A}\ge 0$. Then it holds that
\begin{align*}
\sum_{i=0}^{t-1}\sum_{j=i+1}^t {\EE{\ip{e_i,e_j}}} \le
 \frac2{\alpha\rhod{P_A}}{\sum}_{i=0}^{t} \E[\normsm{e_i}^2 ]\,.
\end{align*}
\end{lemma}
\begin{proof}
Let $j>i$. Then, 
\begin{align*}
\EE{\ip{e_i,e_j}}
&=\E \ip{e_i, F_{j,i+1} e_i+\alpha\textstyle\sum_{k=i+1}^j F_{j,k+1}\zeta_{k} }\\
&=\E\ip{e_i,F_{j,i+1} e_i}  &\text{(from \cref{lem:mart}\ref{lem:mart:noisecancel})}\\
&=\E\ip{e_i, (I-\alpha A)^{j-i} e_i}\,. & \text{(from \cref{lem:mart}\ref{lem:mart:unroll})}
\end{align*}
Now, from the definition of spectral norms, $\ip{e_i, (I-\alpha A)^{j-i} e_i } \le \norm{e_i}^2 \norm{(I-\alpha A)^{j-i}}
\le \norm{e_i}^2 \norm{ I-\alpha A }^{j-i}$.
By definition, $\rhod{P_A} = \lmin{ A + A^\top - \alpha A^\top A }$.
Using this, and the definition of spectral norms,
\begin{align*}
\norm{I-\alpha A}^2 = \sup_{x:\norm{x}=1} \norm{ (I-\alpha A) x }^2 = \sup_{x:\norm{x}=1} x^\top (I-\alpha A)^\top (I-\alpha A) x = 1- \alpha \rhod{P_A}\,.
\end{align*}
Note that $1-\alpha \rhod{P_A}\ge 0$ as the left-hand side of the above equality is obviously nonnegative.
Now, by the elementary inequality $\sqrt{1-x} \le 1- \frac{x}{2}$ which holds as long as $1-x\ge 0$,
$\norm{I-\alpha A} \le 1- \frac{\alpha \rhod{P_A}}{2}$. Putting things together,
\begin{align*}
\ip{e_i, (I-\alpha A)^{j-i} e_i } \le \left(1- \tfrac{\alpha \rhod{P_A}}{2} \right)^{j-i} \norm{e_i}^2 \,.
\end{align*}
Thus, using $\sum_{i=1}^\infty \gamma^i 
= \gamma/(1-\gamma) \le 1/(1-\gamma)$ which holds for any $\gamma\in (0,1)$
and since by assumption $\rhod{P_A}\ge 0$,
\begin{align*}
\label{inter}
\MoveEqLeft 
\sum_{i=0}^{t-1}\sum_{j=i+1}^t {\EE{\ip{e_i,e_j}}}
=\sum_{i=0}^{t-1}\sum_{j=i+1}^t \left(1-\frac{\alpha\rhod{P_A}}{2}\right)^{j-i} \E[\norm{e_i}^2]
\leq \sum_{i=0}^{t-1} \E[\norm{e_i}^2] \sum_{j=i+1}^\infty \left(1-\frac{\alpha\rhod{P_A}}{2}\right)^{j-i} \\
&\leq \sum_{i=0}^{t-1} \E[\norm{e_i}^2] \frac{2}{\alpha \rhod{P_A}}
=\frac2{\alpha\rhod{P_A}} {\sum}_{i=0}^{t-1}\E[\normsm{e_i}^2]
\leq \frac2{\alpha\rhod{P_A}}{\sum}_{i=0}^{t}\E[\normsm{e_i}^2]\,,
\end{align*}
finishing the proof.
\end{proof}


\begin{theorem}\label{th:pdrate}
Assume that $\rhos{P_A}\ge 0$. Then,
\begin{align}
%\EE{\norm{\eh_t}^2}
%\leq
%\left(1+\frac2{\alpha\rhod{P_A}}\right)\frac{1}{\alpha \rhos{P_A}} \left(\frac{\norm{e_0}^2}{(t+1)^2}+ \frac{\alpha^2{\sigma}_1^2+ {\sigma}_2^2 (\alpha^2\norm{\ts}+\alpha \norm{e_0})}{t+1} \right)\,.
\E[\norm{\eh_t}^2]
\leq \left(1+\frac4{\alpha\rhod{P_A}}\right)\, \frac2{\alpha\rhos{P_A}}\, \,
%\left(\frac{\norm{e_0}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_P^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_P^2\norm{\ts})\norm{e_0}}{t+1} \right)\,.
\left(\frac{\norm{e_0}^2}{(t+1)^2}+ \frac{\alpha^2 \,\sigma_1^2}{t+1} \right)\,.
\end{align}
\end{theorem}
\begin{proof}
As it was noted earlier, $\rhod{P_A}\ge \rhos{P_A}$. Hence, it follows that $\rhod{P_A}\ge 0$ holds, too
and we can use  \cref{lem:crosstermbound}.
Combining \eqref{eq:ehtexp} and \cref{lem:crosstermbound} we get
\begin{align*}
\EE{\normsm{\eh_t}^2} \le  
\left( 1+\frac4{\alpha\rhod{P_A}} \right)\,
\frac{1}{(t+1)^2} \, \sum_{i=0}^{t}\E[\normsm{e_i}^2]\,.
\end{align*}
Expanding $\norm{e_i}^2$ using \eqref{eq:etft},
using $(a+b)^2\leq 2 a^2+2 b^2$ which holds for any $a,b\in \R$,
\begin{align*}
\E[\normsm{e_i}^2]
&\le  2 \EE{\ip{F_{i,1}e_0,F_{i,1}e_0}}+2 \alpha^2\sum_{j=1}^i\EE{\ip{ F_{i,j+1}\zeta_j, F_{i,j+1}\zeta_j}}\\
& \le 2 (1-\alpha\rhos{P_A})^i\norm{e_0}^2+ 2 \alpha^2\frac{{\sigma}_1^2}{\alpha \rhos{P_A}}\,,
\end{align*}
where the second inequality follows from 
 \cref{lem:mart}\ref{lem:mart:innerproduct}, $\E[ \normsm{\zeta_j}^2 ] \le \sigma_1^2$
 and from  $\sum_{j=0}^{i-1} (1-\alpha \rhos{P_A})^j\leq \frac{1}{\alpha \rhos{P_A}}$. 
 Hence, using again the sum of geometric series,
\begin{align*}
\sum_{i=0}^t \E[ \norm{e_i}^2 ] 
& \le
\frac{ 2 \alpha^2{\sigma}_1^2}{\alpha \rhos{P_A}}\,(t+1) +
 2 \norm{e_0}^2 \sum_{i=0}^t(1-\alpha\rhos{P_A})^i  
 \le
\frac{ 2 \alpha^2 {\sigma}_1^2}{\alpha \rhos{P_A}}\,(t+1) +
 \frac{2 \norm{e_0}^2}{\alpha\rhos{P_A}} \,.
\end{align*}
Putting things together,
\begin{align*}
\E[\norm{\eh_t}^2]
\leq \left(1+\frac4{\alpha\rhod{P_A}}\right)\, \frac2{\alpha\rhos{P_A}}\, \,
%\left(\frac{\norm{e_0}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_P^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_P^2\norm{\ts})\norm{e_0}}{t+1} \right)\,.
\left(\frac{\norm{e_0}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_1^2)}{t+1} \right)\,.
\end{align*}
\end{proof}

\subsection{The case of Hurwitz $A$}
To prove \cref{th:rate} we first show the existence of a nonsingular matrix $U\in \R^{d\times d}$ and a constant
$\alpha_{P_U}$ which depends only on the distribution $P_U$ of $U^{-1} A_t U$ such that for all $\alpha\in (0,\alpha_{P_U})$, $\alpha \rhos{P_U}>0$. This will be used together with a change of basis to finish the proof.

We start with a lemma that shows that if $A+A^\top$ is PD then $U=I$ works with a suitable constant $\alpha_P>0$:
\begin{lemma}\label{lm:pd}
Assume that $A+A^\top$ is PD. Then there exists a constant $\alpha_P>0$ such that  $\rhos{P_A}>0$ 
holds for all $\alpha \in (0,\alpha_P)$.
\end{lemma}
\begin{proof}
We compute
\begin{align*}
\rhos{P_A}&=\inf_{x:\norm{x}=1}x^\top (A^\top+A)x -\alpha x^\top\EE{A_t^\top A_t} x
	& (\text{by \cref{def:spect}}) \\
&=\inf_{x:\norm{x}=1}x^\top (A^\top+A)x -\alpha x^\top A^\top A\, x -\alpha x^\top \EE{M_t^\top M_t} x
	& (\text{using }A_t = A+M_t, \EE{M_t}=0) \\
&\geq \lambda_{\min}(A^\top+A)-\alpha( \norm{A}^2+ \sigma^2_{A_P})\,,
\end{align*}
where the last inequality follows from optimizing $x$ separately for all the three term,
$\lmin{M} = \inf_{x:\norm{x}=1} x^\top M x$ which holds for any real symmetric matrix $M$,
the definition of the spectral norm and 
 that $\sup_{x:\norm{x}=1} x^\top \EE{M_t^\top M_t} x
\le \EE{ \sup_{x:\norm{x}=1} x^\top M_t^\top M_t x} = \EE{\sup_{x:\norm{x}=1} \norm{M_t x}^2 }
=\EE{ \norm{M_t}^2 } \le \sigma^2_{A_P}$.
The proof is complete by choosing $\alpha_P=\frac{\lambda_{\min}(A^\top+A)}{\norm{A}^2+\sigma^2_A}$
and noting that $\alpha_P$ is positive because by assumption $\lambda_{\min}(A^\top+A)>0$.
\end{proof}

Now assume that $A$ is Hurwitz. By \cref{lm:simtran} there exists a nonsingular matrix $U\in \R^{\dcd}$ such that
$B = U^{-1} A U$ is so that $B+B^\top$ is PD. 
Choose a matrix $U$ with this properties. 
Let $B_t = U^{-1} A_t U$ and let $P_U$ be the common distribution of $\{B_t\}_{t\ge 1}$. 
Note that $\EE{B_t} = B$.
Premultiplying both equations in \eqref{eq:lsa} by $U^{-1}$, we get
\begin{subequations}\label{eq:lsa2}
\begin{align}
\label{conststep2}&\text{LSA:} &U^{-1} \theta_t&=U^{-1} \theta_{t-1}+\alpha(U^{-1} b_t-B_t U^{-1} \theta_{t-1})\,,\\
\label{iteravg2}&\text{Average:} &U^{-1} \thh_t&=\frac{1}{t+1}{\sum}_{s=0}^{t}\,U^{-1} \theta_s\,.
\end{align}
\end{subequations}
Defining $\gamma_t = U^{-1} \theta_t$ and $\hat \gamma_t = U^{-1}\thh_t$, we see that
\begin{align*}
\gamma_t =\gamma_{t-1}+\alpha(U^{-1} b_t-B_t \gamma_{t-1})\,, \qquad\qquad
\hat \gamma_t =\frac{1}{t+1}{\sum}_{s=0}^{t}\,\gamma_s \,,
\end{align*}
which is a CALSA update applied to the data $\{(B_t,U^{-1} b_t)\}_{t\ge 1}$. 
Since $B+B^\top$ is PD,
by \cref{lm:pd} there exist a constant $\alpha_{P_U}>0$ such that for any $\alpha\in (0,\alpha_{P_U})$,
$\rhos{P_U}>0$. Take any such $\alpha$.

Defining $\gamma_* = U^{-1} \ts$, we see that $B \gamma^* = U^{-1} b$.
Let $z_t = \gamma_t - \gamma_*$ and $\hat z_t = \hat\gamma_t - \gamma_*$.
Define $\sigma_z^2 $
to be an upper bound on $\EE{ \norm{U^{-1} (b_t-b) + (B_t-B) \gamma_* }^2 }$.
Note that $U^{-1} (b_t-b) + (B_t-B) \gamma_* = 
U^{-1} \left\{(b_t-b) +(A_t-A) \ts\right\}$, hence, 
we can choose $\sigma_z^2 = \norm{U^{-1}}^2 \sigma_1^2$, where $\sigma_1^2$ is an upper bound on
$\EE{ \norm{ (b_t-b) +(A_t-A) \ts }^2 }$.
Since $\rhos{P_U}>0$, by \cref{th:pdrate} it holds that
\begin{align*}
\E[\norm{\hat z_t}^2]
\leq 
\left(1+\frac4{\alpha\rhod{P_U}}\right)\, \frac2{\alpha\rhos{P_U}}\, \,
\left(\frac{\norm{z_0}^2}{(t+1)^2}+ \frac{\alpha^2 \,\sigma_z^2}{t+1} \right)\,.
\end{align*}
Furthermore,
\begin{align*}
\E[ \norm{\eh_t}^2 ] = \E[ \norm{ U \hat z_t}^2 ] \le \norm{U}^2 \E[ \norm{\hat z_t}^2 ]\,,
\qquad
\text{and}
\qquad
\norm{z_0}^2 \le \norm{U^{-1}}^2 \norm{e_0}^2\,,
\end{align*}
Putting things together we get 
\begin{align*}
\E[ \norm{\eh_t}^2 ] \le
\cond{U}^2 
\left(1+\frac4{\alpha\rhod{P_U}}\right)\, \frac2{\alpha\rhos{P_U}}\, \,
\left(\frac{\norm{e_0}^2}{(t+1)^2}+ \frac{\alpha^2 \,\sigma_1^2}{t+1} \right)\,,
\end{align*}
which gives the desired result:

\thrate*

\begin{comment}
\begin{theorem}[Lower Bound]
There exists a distribution $P$ over $\R^{\dcd}\times\R^d$ satisfying \Cref{assmp:lsa}, such that $\alpha_P>0$, such that $\rhos{P_A}>0$ and $\rhod{P_A}>0,\,\forall \alpha\in (0,\alpha_P)$ and
\begin{align*}
\EE{\norm{\thh_t-\ts}^2} &\geq \frac{1}{(t+1)^2}(\alpha)^{-2}(\rhod{P_A}\rhos{P_A})^{-1}\Big( \big(1-(1-\alpha \rhos{P_A})^t\big) \norm{\theta_0-\ts}^2 \\&+ \sum_{s=1}^t \big(1-(1-\alpha \rhos{P_A})^{(t-s)}\big) \big(\alpha^2(\sigma_A^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_A^2\norm{\ts})\norm{\theta_0-\ts}\big) \Big)
\end{align*}
\end{theorem}
\end{comment}
\thlb*
\begin{proof}
Let $d=2$ and let
$A=\begin{bmatrix} \lambda_{1} &0\\ 0& \lambda_{2}\end{bmatrix}$, 
for some $\lambda_{2}>\lambda_{1}>0$ and let $\{N_t\}_{t\ge 1}\subset \R$ be an i.i.d. sequence of zero-mean random variables with $\sigma^2 = \EE{ N_t^2 }<+\infty$.
Consider the LSA with $(A_t,b_t)\sim P$ such that 
$A_t=A$ for all $t\ge 1$ 
and $b_t = (N_t,0)^\top$.
Note that in this example $\ts=0$, $\sigma_{A_P}^2 = 0$, $A_P=A$ and $\sigma_{b_P}^2 = \sigma^2$.
We calculate $\eh_t$ to be
\begin{align*}
\eh_t&=\frac{1}{t+1}\sum_{s=0}^t e_t = \frac{1}{t+1}\sum_{s=0}^t (I-\alpha A)^{t-s} e_0 +\alpha \sum_{s=1}^t \sum_{i=s}^t (I-\alpha A)^{i-s} b_s\\
&=\frac{1}{t+1}(\alpha A)^{-1}\left[\left(I-(I-\alpha A)^{t+1}\right)e_0 + \alpha \sum_{s=1}^t \left(I-(I-\alpha A)^{t+1-s}\right) b_s\right]\,.
\end{align*}
Denote by $x_i$ the $i$th entry of a vector $i$.
Owning to the diagonal structure of $A$ and that $b_{s,2}=0$,
the two components, $\eh_{t,1}$ and $\eh_{t,2}$ of the vector $\eh_t$ are calculated to satisfy
\begin{align*}
\eh_{t,1} & =  \frac{1}{t+1} \frac{1-(1-\alpha \lambda_1)^{t+1}}{ \alpha \lambda_1} e_{0,1}
					+ \frac{1}{t+1} \sum_{s=1}^t (1-(1-\alpha \lambda_1)^{t+1-s}) N_s\,,\\
\eh_{t,2} & =  \frac{1}{t+1} \frac{1-(1-\alpha \lambda_2)^{t+1}}{ \alpha \lambda_2} e_{0,2}\,.
\end{align*}
Now, choosing $U=I$, $\rhos{P_U} = \rhos{P_A} = \min( \lambda_1(2-\alpha \lambda_1), \lambda_2 (2-\alpha \lambda_2) )$ which is positive as long as $0<\alpha \lambda_2<2$ (which, in this case, also happens to be the necessary condition for the errors to decay).
Setting $\lambda \defeq \lambda_1=\lambda_2$,
$\alpha_P = 1/\lambda$ we see that 
$\rhos{P_A}=\rhod{P_A} = \lambda(2-\alpha \lambda)$ (the reason for choosing $\alpha_P$ this way will become clear later). \todoc{The claim that for $\lambda_1<\lambda_2$, $\rhos{P_A} = \lambda_1(2-\alpha \lambda_1)$ was false.}
Thanks to $\EE{N_s}=0$,
\begin{align*}
\EE{\eh^2_{t,1}} =
 \frac{1}{(t+1)^2} \left(\frac{1-(1-\alpha \lambda)^{t+1}}{ \alpha \lambda}\right)^2 e_{0,1}^2
					+ \frac{ \sigma^2 \sum_{s=1}^t (1-(1-\alpha \lambda)^{t+1-s})^2 }{(t+1)^2}\,,
\end{align*}
while
\begin{align*}
\EE{\eh^2_{t,2}} = \eh^2_{t,2} = 
\frac{1}{(t+1)^2} \left(\frac{1-(1-\alpha \lambda)^{t+1}}{ \alpha \lambda}\right)^2 e_{0,2}^2\,.
\end{align*}
For $0<\alpha<\alpha_P$, $\alpha\lambda \le \alpha \rhos{P_A} \le 2\alpha \lambda$, hence 
\begin{align*}
1-(1-\alpha \lambda)^i &\ge 1-\left(1-\tfrac12 \alpha \rhos{P_A}\right)^i\,, \quad \alpha \in (0,\alpha_P), \,\, i \ge 0\,\\
(\alpha \lambda)^{-1} &\ge  (\alpha \rhos{P_A})^{-1}\,.
\end{align*}
Thus, defining $\beta_i = 1-\left(1-\tfrac12 \alpha \rhos{P_A}\right)^i$,
\begin{align*}
\EE{ \norm{\eh_t}^2 } 
& = \EE{\eh^2_{t,1}} + \EE{\eh^2_{t,2}} 
   \ge \frac{1}{(t+1)^2} 
   \left(\frac{\beta_{t+1}}{ \alpha \rhos{P_A}}\right)^2 \norm{e_0}^2
   +
    \frac{ \sigma^2 \sum_{s=1}^t \beta_{s}^2 }{(t+1)^2}\,,
\end{align*}
finishing the proof.
%Here $(a)$ and $(b)$ follows from the \iid assumption. Note that in this example, $\rhos{P_A}=\rhod{P_A}=2\lambda_{\min} -\alpha \lambda_{\min}^2=\lambda_{\min}(2-\alpha \lambda_{\min})$, and $\norm{\ts}=0$ and $\sigma^2_A=0$. Further, the result follows by noting the fact that  $\norm{b_t}^2=b_t(1)^2$ and $\norm{\theta_t}^2=\theta_t(1)^2$.
%\begin{align}
%\EE{\norm{\eh_t}^2}\geq \frac{1}{(t+1)^2}(\alpha \lambda_{\min})^{-2}(1-\alpha \lambda_{\min}^t) \theta^2_0(1)
%\end{align}
\end{proof}

Note that it may seem silly to consider $d=2$ and use a diagonal matrix with identical elements.
The reason we considered $d=2$ above is because the proof also leads to the stronger conclusion
that 
\begin{align}
\inf_P \sup_{\alpha: \alpha \rhos{P_A}>0} \alpha \rhos{P_A} = 0\,,
\label{eq:badrhos}
\end{align}
where the infimum over $P$ is for those
instances where $\lambda_2$ is fixed and $0<\lambda_1<\lambda_2$ is allowed to change.
Indeed, when $\lambda_1<\lambda_2$, $\alpha_P = 2/\lambda_2$ and \eqref{eq:badrhos} holds (choosing $\lambda_1\to 0$). 
When \eqref{eq:badrhos} holds we see that the lower bound blows up for any fixed choice of $\alpha$ in a worst-case sense. Thus, already in the two-dimensional, diagonal case, there is no ``universal'' stepsize.

\begin{comment}
\textbf{Proof of }
Given any $A_P$ we can choose a unitary matrix $U$ (i.e., $U^\top U=I$) such that $U^\top A_P U=D_P$, where $D_P$ is a diagonal matrix. Thus with $\gamma=U\top \theta$, we have
\begin{align}
\theta_{t}=(I-\alpha A_P)\theta_{t-1}+\alpha b_t\\
\label{eq:gamrec} \gamma_t=(I-\alpha D_P)\gamma_{t-1}+\alpha \zeta_t,
\end{align}
where $\zeta_t=U^\top b_t$. Note that the unitary transformation preserves norm, i.e., $\norm{\gamma}=\theta^\top UU^\top \theta=\norm{\theta}$, $\norm{\theta}_A=\norm{\gamma}_{\Lambda}$ and $\EE{\zeta_t\zeta_t^\top}=\EE{U^\top b_t b_t^\top U}\leq D_P$. Since, $\Lambda_P$ is a diagonal, \eqref{eq:gamrec} has $d$ separate $1$-dimensional equations. In what follows, without loss of generality, all quantities are in $1$-dimension. Also, $\ts=\gamma_*=\mathbf{0}$. Also, let $e_0=\gamma_0$
\begin{align}
\eh_t=\frac{1}{t+1}(\alpha \Lambda)^{-1}\big([I-(I-\Lambda D)^{t+1}]e_0
+\alpha\sum_{s=1}^t [I-(I-\alpha \Lambda)^{t+1-s}] \zeta_s]\big)
\end{align}
Now for small $t$ such that $\alpha \Lambda t<I$,  we have $I- (I-\alpha \Lambda)^t\approx \alpha \Lambda t$. Thus, we have 
\begin{align}
\eh_t \approx \frac{1}{t+1}\big((t+1)e_0
+\alpha\sum_{s=1}^t (t+1-s) \zeta_s]\big)
\end{align}
Taking expectation, we have
\begin{align}
\EE{\norm{\eh_t}}&\approx \frac{1}{(t+1)^2} \big(\B (t+1)^2+\alpha^2 \sum_{s=1}^{t} s^2 \sigma^2_{b_P}),
%\EE{\norm{\eh_t}_\Lambda}&\approx \frac{1}{(t+1)^2} \big( (t+1)e_0^\top \Lambda e_0 +\sum_{s=1}^{t} s^2 \zeta_t^\top \Lambda \zeta_t)\\
\end{align}
The $\sum_{s=1}^t s^2=O(t^3)$, and hence we have $\EE{\norm{\eh_t}}\approx \frac{1}{(t+1)^2} \big(\B (t+1)^2+\alpha^2 O(t^3) \sigma^2_{b_P})$. 
Similarly, we have

$\EE{\norm{\eh_t}_\Lambda}\approx \frac{1}{(t+1)^2} \big((t+1)^2 \B \Lambda  +\alpha^2 O(t^3) \sigma^2_{b_P})$. Now, using $\alpha \Lambda t <1$, it follows that $\EE{\norm{\eh_t}_\Lambda}\approx \frac{1}{(t+1)^2} \big((t+1)^2\B \Lambda  +\alpha^2 O(t^3) \sigma^2_{b_P} \Lambda)\approx O(\frac{1}{\alpha (t+1)}+\alpha \sigma^2_{b_P})$. Further, in the case of $\SPDSN$, it follows $\sigma^2_{b_P}<\Lambda$, we have $\EE{\norm{\eh_t}_\Lambda}\approx \frac{1}{(t+1)^2} \big((t+1)^2\B \Lambda  +\alpha^2 O(t^3) \sigma^2_{b_P} \Lambda)\approx O(\frac{1}{\alpha (t+1)}+\frac{1}{t})$.

\end{comment}

\section{Proof of \cref{th:pspd}}
\label{sec:pspd}
\thpspd*
\begin{proof}
For any $P$ in $\SPD$ or $\SPDSN$, \todoc{Class names need to be updated and we don't have (i), (ii) and (iii) anymore.} the corresponding $A_P$ matrix is real symmetric and positive definite. Thus, for each problem instance there exists an orthogonal matrix $U$ (i.e., $U^\top U=I$) such that $U^\top U=I$ and $U^\top A_P U=\Lambda_P$. Define $\zeta_t\eqdef U^\top b_t$, and $\gamma=U^\top \theta$. Now, we have
\begin{align}
\theta_t&=(I-\alpha A_P)\theta_{t-1}+\alpha b_t\,,\\
\label{eq:gamrec}\gamma_t&=(I-\alpha \Lambda_P)\gamma_{t-1}+\alpha \zeta_t\,.
\end{align}
Since $U$ is an orthogonal matrix $\norm{\gamma}_{\Lambda_P}=\gamma^\top \Lambda_P \gamma= \theta^\top U \Lambda_P U^\top \theta=\norm{\theta}_{A_P}$ and similarly $\norm{\gamma}=\norm{\theta}$ and when $P\in \P_{SPDSN}$ we have $\EE{\zeta_t \zeta_t^\top}\leq \Lambda_P$. Thus, for any adversarial choice of $A_P$, it is equivalent to choosing a problem corresponding to $\Lambda_P$. Thus, in order to prove the bounds it is enough to choose such diagonal problems. In what follows, we let $e_t\eqdef\gamma_t-\gamma_*$ (though we have used $e_t=\theta_t-\ts$ everywhere else, we have re-defined to avoid introducing additional notation).

It is clear that \eqref{eq:gamrec}, has $d$ separate $1$-dimensional equations. Further, the MSE is a summation of $d$ separate terms (i.e. $\EE{\norm{\eh_t}}=\sum_{i=1}^d \eh_t(i)^2$, $\EE{\norm{\eh_t}_{\Lambda_P}}=\sum_{i=1}^d \Lambda(i) \eh_t(i)^2$, where $\Lambda(i),i=1,\ldots, d$ are the eigenvalues) and any adversarial choice will maximize each of the $d$ terms. Thus, it follows that such an adversarial choice should have $\Lambda(i)=\lambda, i=1,\ldots, d$. Thus, it is enough to prove the bounds for the $1$-dimensional case, and multiply them with $d$ for the general case. In what follows, we deal with the $1$-dimensional case, where $\lambda\in (0,1)$ is the eigenvalue, and $b_t\in \R$ is the noise sequence and the condition $\EE{b_tb_t^\top}\leq A_P$ translates to $\EE{b_t^2}\leq \lambda$. In the $1$-dimensional case, we have the following relation for the error
\begin{align}\label{eq:onederr}
\eh_t=\frac{1}{t+1}\left[\sum_{s=0}^t (1-\alpha \lambda)^s e_0+ \alpha \sum_{i=1}^t \sum_{s=0}^{t-i} (1-\alpha \lambda)^s b_i\right]\,.
\end{align}
We now look at cases $(i)$, $(ii)$ and $(iii)$, and prove matching lower and upper bounds. In what follows, we use the facts
\begin{enumerate}
\item For sufficiently large $t$ we have $(1-\frac{1}{t+1})^t\geq c$ (the constant $c\approx \frac{1}{e}$, where $e$ is the base of the natural logarithm). This will be used in the lower bound proofs.
\item $\sum_{s=0}^t(1-\alpha \lambda)^2\leq \min\{(\alpha\lambda)^{-1},(t+1)\}$. This will be used in the upper bound proofs.
 \end{enumerate}
\textbf{Lower Bound for $\eps_t(\SPD)$:}
 For sufficiently large $t$, and any step-size$\alpha \in (0,1)$ pick $\lambda$ such that $\alpha \lambda (t+1)=1$. Taking expectation in \eqref{eq:onederr}, and letting $\B=e_0^2$, we have
\begin{align}\label{eq:intererr}
\EE{\norm{\eh_t}^2}=\frac{1}{(t+1)^2}\left[ (\sum_{s=0}^t(1-\alpha \lambda)^s)^2 \B+ \alpha^2\sigma^2_{b_P} \sum_{i=1}^t (\sum_{s=0}^{t-i} (1-\alpha \lambda)^s)^2\right],
\end{align}
where we use the property of independence. Now for this choice of $\lambda$ we have
\begin{align}
\EE{\norm{\eh_t}^2}&\geq \frac{1}{(t+1)^2}\left[ c^2(t+1)^2 \B+ \alpha^2\sigma^2_{b_P} c^2\sum_{i=1}^t (i)^2\right]
\geq c^2\left[B+ \alpha^2 \sigma^2_{b_P} \Theta(t) \right],
\end{align}
which completes the lower bound part for this case.

\textbf{Upper Bound for $\eps_t(\SPD)$:}
Starting from \eqref{eq:intererr}, we have
\begin{align}
\EE{\norm{\eh_t}^2}
&\leq \frac{1}{(t+1)^2}\left[\B (\min\{(\alpha\lambda)^{-1},(t+1)\})^2+ \alpha^2 \sigma^2_{b_P} (t+1) (\min\{(\alpha\lambda)^{-1},(t+1)\})^2 \right]\\
&= \left[\B (\min\{\frac{1}{\alpha \lambda (t+1)},1\})^2
		+ \alpha^2 \sigma^2_{b_P} (t+1) (\min\{\frac{1}{\alpha \lambda (t+1)},1\})^2 \right]\\
&\leq \left[\B + \alpha^2 \sigma^2_{b_P} (t+1) \right]\,, 
\end{align}
which completes the upper bound part for this case.

\textbf{Lower Bound for $\eps_t'(\SPD)$:}
We will slightly abuse notation by writing $\norm{\eh_t}^2_{\lambda} = \lambda (\eh_t)^2$
and writing $\norm{\eh_t}^2 = (\eh_t)^2$.
For sufficiently large $t$, and any step-size $\alpha \in (0,1)$ pick $\lambda$ such that $\alpha \lambda (t+1)=1$. Taking expectation in \eqref{eq:onederr}, and letting $\B=e_0^2$, we have
\begin{align}\label{eq:intererr}
\EE{\norm{\eh_t}^2_{\lambda}}=\frac{1}{(t+1)^2}\left[ \lambda (\sum_{s=0}^t(1-\alpha \lambda)^s)^2 \B+ \alpha^2\sigma^2_{b_P}\lambda \sum_{i=1}^t (\sum_{s=0}^{t-i} (1-\alpha \lambda)^s)^2\right],
\end{align}
where we use the property of independence. Now for this choice of $\lambda$ we have
\begin{align}
\EE{\norm{\eh_t}^2}&\geq \frac{1}{(t+1)^2}\left[ \lambda c^2(t+1)^2 \B+ \lambda \alpha^2\sigma^2_{b_P} c^2\sum_{i=1}^t (i)^2\right]
= c^2\left[\frac{\B}{\alpha (t+1)}+ \alpha  \sigma^2_{b_P} \Theta(1) \right],
\end{align}
which completes the lower bound part for this case.

\textbf{Upper Bound for $\eps_t'(\SPD)$:}
Starting from \eqref{eq:intererr}, we have
\begin{align}
\EE{\norm{\eh_t}^2_{\lambda}}&\leq \frac{1}{(t+1)^2}\left[\lambda \B (\min\{(\alpha\lambda)^{-1},(t+1)\})^2+ \lambda \alpha^2 \sigma^2_{b_P} (t+1) (\min\{(\alpha\lambda)^{-1},(t+1)\})^2 \right]\\
&= \left[\B (\min\{\frac{1}{\alpha \lambda (t+1)},1)^2+ \lambda \alpha^2 \sigma^2_{b_P} (t+1) (\min\{\frac{1}{\alpha \lambda (t+1)},1)^2 \right]\,.
\end{align}
Here if $\min{(\alpha\lambda)^{-1},(t+1)}=\frac{1}{\alpha \lambda}$, then we have $\lambda(\min{(\alpha\lambda)^{-1},(t+1)})^2=\frac{1}{\alpha^2 \lambda}\leq \frac{(t+1)}{\alpha}$, and hence $\frac{1}{(t+1)^2}\lambda(\min{(\alpha\lambda)^{-1},(t+1)})^2=\frac{1}{\alpha (t+1)}$. In the other case, when $\min{(\alpha\lambda)^{-1},(t+1)}=(t+1)$, we have
$\frac{1}{(t+1)^2}\lambda(\min{(\alpha\lambda)^{-1},(t+1)})^2=\frac{1}{\alpha (t+1)}$. Thus
\begin{align}
\EE{\norm{\eh_t}^2_{\lambda}}&\leq \left[\B \frac{1}{\alpha (t+1)}+ \alpha \sigma^2_{b_P} \right]\,.
\end{align}
This completes the upper bound part for this case.

\textbf{Lower Bound for $\eps_t'(\SPDSN)$:}
For sufficiently large $t$, and any step-size$\alpha \in (0,1)$ pick $\lambda$ such that $\alpha \lambda (t+1)=1$. Taking expectation in \eqref{eq:onederr}, and letting $\B=e_0^2$, we have
\begin{align}\label{eq:intererr}
\EE{\norm{\eh_t}^2_{\lambda}}=\frac{1}{(t+1)^2}\left[ \lambda (\sum_{s=0}^t(1-\alpha \lambda)^s)^2 \B+ \alpha^2\sigma^2_{b_P}\lambda \sum_{i=1}^t (\sum_{s=0}^{t-i} (1-\alpha \lambda)^s)^2\right],
\end{align}
where we use the property of independence. Now for this choice of $\lambda$ we have
\begin{align}
\EE{\norm{\eh_t}^2}&\geq \frac{1}{(t+1)^2}\left[ \lambda c^2(t+1)^2 \B+ \lambda \alpha^2 \lambda c^2\sum_{i=1}^t (i)^2\right]
= c^2\left[\frac{\B}{\alpha (t+1)}+   \Theta(\frac{1}{t}) \right],
\end{align}
where we use $\sigma^2_{b_P}=\lambda$. This completes the lower bound part for this case.

\textbf{Upper Bound for $\eps_t'(\SPDSN)$:}
Starting from \eqref{eq:intererr}, we have
\begin{align}
\EE{\norm{\eh_t}^2_{\lambda}}
&\leq \frac{1}{(t+1)^2}\left[\lambda \B 
	(\min\{(\alpha\lambda)^{-1},(t+1)\})^2
		+ \lambda \alpha^2 \sigma^2_{b_P} (t+1) (\min\{(\alpha\lambda)^{-1},(t+1)\})^2 \right]\\
&= \left[\B 
	(\min\{\frac{1}{\alpha \lambda (t+1)},1\})^2
		+ \lambda \alpha^2 \lambda (t+1) (\min\{\frac{1}{\alpha \lambda (t+1)},1\})^2 \right]\,.
\end{align}
Here if 
$\min\{(\alpha\lambda)^{-1},(t+1)\} = \frac{1}{\alpha \lambda}$, 
then we have 
$\lambda(\min\{(\alpha\lambda)^{-1},(t+1)\})^2
	=\frac{1}{\alpha^2 \lambda}\leq \frac{(t+1)}{\alpha}$, 
	and hence 
$\frac{1}{(t+1)^2} \lambda(\min\{(\alpha\lambda)^{-1},(t+1)\})^2
						=\frac{1}{\alpha (t+1)}$. 
In the other case, when $\min\{(\alpha\lambda)^{-1},(t+1)\}=(t+1)$, we have
	$\frac{1}{(t+1)^2}\lambda(\min\{(\alpha\lambda)^{-1},(t+1)\})^2=\frac{1}{\alpha (t+1)}$. 
Thus,
\begin{align}
\EE{\norm{\eh_t}^2_{\lambda}}
	&\leq \left[\B \frac{1}{\alpha (t+1)}+ \alpha \frac{1}{\alpha (t+1)} \right]\,,
\end{align}
where we use $\sigma^2_{b_P}=\lambda$. This completes the upper bound part for this case.
\end{proof}
\begin{comment}

It is trivial to check that any $\alpha \in (0,1)$ is a universal step-size for $\P_{USN}$ and $\P_{SN}$. Now for a given $t$, pick $A_P\in (0,1)$ such that $\alpha A_P t=1$. Also, note that $\ts=0$ for any problem in $\P_{USN}$ and $\P_{SN}$. Now, let $\eh_t\eqdef \thh_t-\ts$, we have

\begin{align}
\eh_t=\frac{1}{t+1}(\alpha A_P)^{-1}\big([I-(I-\alpha A_P)^{t+1}]e_0
+\alpha\sum_{s=1}^t [I-(I-\alpha A_P)^{t+1-s}] b_s]\big)
\end{align}
 We have for $t\geq 0$ such that $\alpha A_P t\leq 1$,  $I- (I-\alpha A_P)^t\approx(\alpha A_P t)$. Thus, we have 
\begin{align}
\eh_t \approx\left(\frac{1}{t+1}[ (t+1)e_0
+\alpha\sum_{s=1}^t (t+1-s)b_s]\right)
\end{align}
Taking expectation, we have with $\B=\norm{e_0}^2$
\begin{align}
\EE{\norm{\eh_t}}\approx\left( \frac{1}{(t+1)^2} [\B (t+1)^2+\alpha^2 \sum_{s=1}^{t} s^2 \sigma^2_{b_P}]\right),
\end{align}
The $\sum_{s=1}^t s^2= O(t^3)$, and hence we have $\EE{\norm{\eh_t}}\approx\left(\frac{1}{(t+1)^2} [\B (t+1)^2+\alpha^2 O(t^3) \sigma^2_{b_P}]\right)$. Similarly, we have 
\begin{align}
\EE{\norm{\eh_t}_{A_P}}\approx\left( \frac{1}{(t+1)^2} [(t+1)^2 \B A_P  +\alpha^2 O(t^3) \sigma^2_{b_P}]\right)
\end{align}
Now, using $\alpha A_P t <1$, it follows that 
\begin{align}
\EE{\norm{\eh_t}_{A_P}}& \approx\left(\frac{1}{(t+1)^2} [(t+1)^2\B A_P  +\alpha^2 O(t^3) \sigma^2_{b_P} A_P]\right) \\
&\approx\left(\frac{1}{\alpha (t+1)}+\alpha \sigma^2_{b_P}\right)
\end{align}
Further, in the case of $\SPDSN$, it follows $\sigma^2_{b_P}<A_P$, we have 
\begin{align}
\EE{\norm{\eh_t}_{A_P}}&=\approx \left(\frac{1}{(t+1)^2} [(t+1)^2\B \Lambda  +\alpha^2 O(t^3) \sigma^2_{b_P} A_P] \right)\\ 
&\approx\left(\frac{1}{\alpha (t+1)}+\frac{1}{t}\right)
\end{align}
\end{comment}

\if0
 \propunistep*

\begin{proof}
The eigenvalues of $I-\alpha A_P$ are $1-\alpha u\pm i v$, and we can ensure that their modulus is less than unity only when $\alpha<\frac{u}{\sqrt{u^2+v^2}}$. By, letting $u\ra 0$ we can make $\alpha\ra 0$.
\end{proof}
\fi

\section{Proofs for \cref{sec:rl}}

\begin{lemma}\label{lm:matnorm}
Let $A$ be a $\dcd$ matrix with $B\ge \max_{ij}\left|A_{ij}\right|$. It follows that $\max_{x\in \R^d: \norm{x}\leq1}x^\top A^\top A x <B^2d$.
\end{lemma}

We now resort to an the following notation for real symmetric positive definite $\dcd$ matrices $C$ and $D$: $C\succeq D$ if $C-D$ is positive definite. We cite the following lemma without proof (see \cite{schur}). The lemma is often attributed to Schur: \todoc{Add citation!}
\begin{lemma}\label{lm:schur}
Let $A,B,C$ $\dcd$ real matrices. Given a symmetric matrix $M=\left[\begin{matrix}A&B \\B^\top &C\end{matrix}\right]$ be a given $2d\times 2d$ matrix, it follows that $M\succeq 0$ if	$A\succeq 0$ and $C-B^\top A^{-1}B\succeq 0$.
\end{lemma}
\if0
\begin{lemma}\label{lm:amat}
Let $\delta\in(0,1)$ a given discount factor and let $\pi$ be a policy. It follows that the matrix $A_\delta=\Phi^\top D_\pi(I-\delta P_\pi)\Phi$ is positive definite ($x^\top Ax>0,\forall x \in \R^d$) $\forall \delta\in(0,1)$.
\end{lemma}
\fi

\thtdadmis*
\begin{proof}
We need to show for the said values of $\alpha$ it holds that $\rhos{P_A}>0$. %This is equivalent to showing $\max_{x\in \R^d: \norm{x}\leq 1}\E{x^\top(I-\alpha A_t)^\top (I-\alpha A_t)x}<1$. 
\todoc{There is an $A_\delta$, whereas I used $\gamma$ for discount in introduction. I don't think $\delta$ is needed.
And now $A_\delta$ is not introduced I think.}

\textbf{Part $(i)$:}  We have $\alpha\phi^\top\phi=\frac{1}{B^2d}\phi^\top_t\phi_t<\beta$ for some $\beta\in (0,1)$. Now, 
\begin{align*}
(A_t+A_t^\top) -\alpha A_t^\top A_t 
& = 2\phi_t(\phi_t-\gamma\phi'_t)^\top -\beta\phi_t(\phi-\gamma\phi'_t)^\top+\beta\gamma\phi'_t(\phi_t-\gamma{\phi'}_t)^\top\\
&= (2-\beta)\phi_t(\phi_t-\gamma\phi'_t)^\top +\beta\gamma\phi'_t\phi^\top_t-\beta\gamma^2\phi'_t{\phi'}_t^\top \\
&= (2-\beta)\phi_t(\phi_t-\gamma\phi'_t)^\top -\beta\gamma^2(\gamma\phi'_t\phi^\top_t- \phi'_t{\phi'}_t^\top)-\beta\gamma(1-\gamma^2)\phi'_t\phi_t^\top\,.
\end{align*}
Taking expectations, we have \todoc{I think there is a syntax error in the next line.}
\begin{align*}
\EE{ (A_t+A_t^\top)-\alpha A_t^\top A_t} &\succeq  A -\beta\gamma^2(A)+\beta\gamma(1-\gamma^2)\EE{\phi'_t\phi_t^\top}
\succeq (1-\gamma^2)[A +\beta\gamma\EE{\phi'_t\phi_t^\top}]\,.
\end{align*}
Now we have 
\begin{align*}
A +\beta\gamma\EE{\phi'_t\phi_t^\top}=& \EE{\phi_t\phi_t}-\gamma\EE{\phi_t{\phi'_t}^\top} +\beta\gamma\EE{\phi'_t\phi_t^\top}
=\EE{\phi_t\phi_t}-\delta\EE{\phi_t{\phi'_t}^\top},
\end{align*}
where $\delta=\gamma(1-\beta)$. 
From our assumption that in $\text{TDON}(B)$, $A_{\delta}=\EE{A_t}$ is PD,
it follows that $x^\top\big(\EE{\phi_t\phi_t}-\delta\EE{\phi_t\phi'_t} )x=x^\top A_{\delta} x>0$.

\textbf{Part $(ii)$:}
In the case of normalized features we have
\begin{align*}
(A_t+A_t^\top) -\alpha A_t^\top A_t 
&= 2\phi_t(\phi_t-\gamma\phi'_t)^\top - (\phi-\gamma\phi'_t)(\phi-\gamma\phi'_t)^\top
=(\phi_t+\gamma\phi'_t)(\phi-\gamma\phi'_t)^\top\\
&=\phi_t\phi_t^\top-\gamma^2\phi'_t{\phi'_t}^\top\,.
\end{align*}
Taking expectations, we have
\begin{align*}
\EE{(A_t+A_t^\top) -\alpha A_t^\top A_t }=\EE{\phi_t\phi_t^\top-\gamma^2\phi'_t{\phi'_t}^\top}
=(1-\gamma^2)\EE{\phi_t\phi_t^\top}
\succeq 0\,.
\end{align*}

\textbf{Part $(iii)$:}
 The GTD updates in \Cref{tb:tdalgos} can be expressed as $x_{t+1}=x_t+\alpha (g_t -H_t x_t)$, where $x_t=\left[\begin{matrix}y_t\\\theta_t\end{matrix}\right]$, $H_t=\left[\begin{matrix}I &A_t \\ -(1-\alpha)A^\top_t & \alpha A_t^\top A_t\end{matrix}\right]$, and $g_t=\left[\begin{matrix} b_t\\ A_t^\top b_t\end{matrix}\right]$, where $A_t=\phi_t(\phi_t-\gamma{\phi'}_t)^\top$. To show that $\alpha=\frac{1}{2B^4 d^2}$ is a universal step-size choice, we need to show that   $\inf_{x\in \R^d:\norm{x}=1}\E{x^\top[(H_t+H_t^\top) -\alpha H_t^\top H_t]x}>0$.

Now $H_t+H_t^\top=\left[\begin{matrix} 2I & \alpha A_t\\ \alpha A_t^\top 2\alpha A_t^\top A_t\end{matrix}\right]$ and $H_t^\top H_t=\left[\begin{matrix} I+(1-\alpha)^2A_tA_t^\top & A_t-\alpha(1-\alpha)A_tA_t^\top A_t\\ A_t^\top-\alpha(1-\alpha)A_t^\top A_t A_t^\top & A_t^\top A_t+\alpha^2A_t^\top A_t A_t^\top A_t\end{matrix}\right]$.  We see that $(H_t+H_t^\top) -\alpha(H_t^\top H_t) \succeq M_t$, \todoc{Clashes with noise $M_t$.}
 where $M_t=\left[\begin{matrix}I &\alpha^2(1-\alpha) A_tA_t^\top A_t\\ \alpha^2(1-\alpha) A_t^\top A_t A_t^\top &\alpha A_t^\top A_t-\alpha^3A_t^\top A_tA_t^\top A_t\end{matrix}\right]$. Now from \Cref{lm:schur}, to show that $M_t\succeq 0$ we need to ensure 
\begin{align*}
\MoveEqLeft 
\alpha A_t^\top A_t -\alpha^3A_t^\top A_t A_t^\top A_t - \alpha^4(1-\alpha)^2A_t^\top A_t A_t^\top A_t A_t^\top A_t\\
&=\alpha A_t^\top\big(I-\alpha^2 A_tA_t^\top-\alpha^3A_tA_t^\top A_tA_t^\top\big)A_t\\
&\succeq \alpha A_t^\top\big(I-\alpha^2 A_t(I+\alpha A_t^\top A_t)A_t^\top\big)A_t\\
&\succeq \frac{\alpha}{2} A_t^\top A_t\,,
\end{align*}
where the last inequality follows from the choice of $\alpha$, bound $B$ on the features and \Cref{lm:matnorm}.
\end{proof}
%\left[\begin{matrix}\end{matrix}\right]