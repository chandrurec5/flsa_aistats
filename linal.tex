%!TEX root =  flsa.tex
\appendix

\section{Proofs for \cref{sec:mainresults}}
\subsection{Hurwitz matrices and positive definiteness}
\label{sec:hurpd}
The symmetric part of a real-valued matrix $A$ is $(A+A^\top)/2$.
The following lemma states that Hurwitz matrices are similar to some matrix whose symmetric part is positive definite.
After the lemma we give an example that shows that the similarity cannot be chosen to be the identity matrix.
\begin{lemma}\label{lm:simtran}
Any Hurwitz matrix is similar to a real matrix whose symmetric part is positive definite.
Furthermore, the similarity transformation can be chosen to be an SPD matrix. \todoc{The reverse is also true, right..?}
\end{lemma}
\begin{proof}
Recall that if $A\in \R^{d\times d}$ is Hurwitz, then there exist a unique SPD matrix $P$ such that the Lyapunov equation
\begin{align*}
A^\top P + P A = I
\end{align*}
is satisfied (e.g., Lemma A.23 of \citet{french2003}).
Take $U = P^{1/2}$. Clearly, $U$ is positive definite and symmetric.
Let $B = U^{-1} A U$. 
Twice the symmetric part of $B$ is 
\begin{align*}
B+B^\top 
 = P^{-1/2} A P^{1/2} + P^{1/2} A^\top P^{-1/2} 
 = P^{-1/2} ( A P +  P A^\top ) P^{-1/2} = P^{-1}\,,
\end{align*}
which is positive definite, hence finishing the proof.
\end{proof}
Now, consider the matrix 
\begin{align*}
A = 
\begin{pmatrix}
1 & -10 \\
0 & 1
\end{pmatrix}\,.
\end{align*}
We claim that this is a Hurwitz matrix, but $A+A^\top$ is not positive definite.
Indeed, $A$ has a single eigenvalue of multiplicity two, which is equal to one. Thus, $A$ is Hurwitz.
On the other hand,
\begin{align*}
A + A^\top = 
\begin{pmatrix}
2 & -10 \\
-10 & 2
\end{pmatrix}\,.
\end{align*}
The eigenvalues of this matrix solve $(2-\lambda)^2 - 100=0$, i.e., they are
$\lambda_1 = 12$ and $\lambda_2 = -8$. Hence, $A+A^\top$ is indeed not SPD.

We note in passing that the change of coordinates argument that we follow here is inspired 
by the paper of \citet{lihong}. 

\subsection{The case when $A_P+A_P^\top$ is positive definite}
In this section we consider \Cref{th:pdrate} when $A=A_P$ is such that its symmetric part is positive definite.
Following the suggestion in the main body of the paper, the general case will be reduced to this case by using 
\cref{lm:simtran}.

Recall that 
\begin{align*}
e_t =  \theta_t-\ts\,, \qquad 
\eh_t = \thh_t-\ts\,, \qquad  \text{and} \qquad 
\zeta_t = b_t-b-(A_t-A_P)\ts\,.
\end{align*}
Further, introduce
\begin{align*}
F_{i,j} & =
\begin{cases}
 (I-\alpha A_i)(I-\alpha A_{i-1}) \dots (I-\alpha A_j)\,, & \text{if } i\ge j\,;\\
 I\,, & \text{otherwise}\,,
\end{cases}
\end{align*}
and let
\begin{align*}
\sigma_1^2 &= \sigma_{A_P}^2\norm{\ts}^2+\sigma_{b_P}^2\,, \qquad  \qquad
\sigma_2^2 = \sigma_{A_P}^2\norm{\ts}\,.
\end{align*}
Note that $\EE{\normsm{\zeta_t}^2}\leq \sigma_1^2$ and $\EE{\md{M_t\zeta_t}}\leq \sigma_2^2$, where recall that $M_t = A_t - A$. \todoc{Why do we have $|\cdot|$ for $M_t \zeta_t$, which is a vector? And why does $\sigma_2^2$ a bound for $\EE{\md{M_t\zeta_t}}$?}

\paragraph{Error Recursion} Let us now look at the dynamics of the error terms defined by
\begin{align}\label{eq:errrec}
\begin{split}
\theta_t&=\theta_{t-1}+\alpha\big(b_t-A_t\theta_{t-1}\big), \text{subtract $\ts$ from both sides})\\
\theta_t-\ts&=\theta_{t-1}-\ts+\alpha\big(b_t-A_t(\theta_{t-1}-\ts+\ts)\big), \text{using definition of $e_t$}\\
e_t&=(I-\alpha A_t)e_{t-1}+\alpha(b_t -b -(A_t-A)\ts), \text{using definition of $\zeta_t$}\\
e_t&=(I-\alpha A_t)e_{t-1}+\alpha\zeta_t
\end{split}
\end{align}

\begin{lemma}\label{lm:pd}
Under \Cref{assmp:lsacmplx} there exists an $\alpha_P>0$ such that $\rhod{P}>0$ and $\rhos{P}>0,~\forall \alpha \in (0,\alpha_P)$.
\end{lemma}
\begin{proof}
\begin{align*}
\rhos{P}&\stackrel{(a)}{=}\inf_{x:\norm{x}=1}x^* (A_P^*+A_P)x -\alpha x^*\EE{A_t^* A_t} x\\
&\stackrel{(b)}{=}\inf_{x:\norm{x}=1}x^* (A_P^*+A_P)x -\alpha x^* A^*_P A_P -\alpha x^* \EE{M_t^* M_t} x\\
&\stackrel{(c)}{\geq} \lambda_{\min}(A^*_P+A_P)-\alpha \norm{A_P}^2-\sigma^2_A
\end{align*}
The proof is complete by choosing $\alpha_P<\frac{\lambda_{\min}(A^*_P+A_P)}{\norm{A_P}^2+\sigma^2_A}$. Above $(a)$ follows from definition of $\rhos{P}$ in \Cref{def:spect}, $(b)$ follows from the fact that $M_t$ is a martingale difference term (see \Cref{assmp:lsacmplx}) and $(c)$ follows from definitions of norms, Jensen inequality, properties of norms and the fact that for a real symmetric matrix $M$ the smallest eigenvalue is given by $\lambda_{\min}=\inf_{x:\norm{x}=1} x^* M x $.
\end{proof}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma}[Product unroll lemma]\label{lem:genunroll}
Let $t>i\ge 1$, $x,y\in \C^d$ be $\F_{i}$-measurable random vectors. Then,
\begin{align*}
\E[x^* F_{t,i+1}y|\F_i]=x^* (I-\alpha A_P)^{t-i} y\,.
\end{align*}
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
By the definition of $F_{t,i+1}$,
and because $F_{t-1,i+1} = (I-\alpha A_{t-1}) \dots (I-\alpha A_{i+1})$ is $\F_{t-1}$-measurable,
as are $x$ and $y$,
\begin{align*}
\EE{x^* F_{t,i+1} y | \F_{t-1} } &= x^\top \EE{ (I-\alpha A_t) | \F_{t-1} } F_{t-1,i+1} y=x^*  (I-\alpha A_P)  F_{t-1,i+1} y\,.
\end{align*}
By the tower-rule for conditional expectations and our measurability assumptions,
\begin{align*}
\EE{x^* F_{t,i+1} y | \F_{t-2} }=x^*  (I-\alpha A_P)  \EE{F_{t-1,i+1} |\F_{t-2}} y= x^* (I-\alpha A_P)^2 F_{t-2,i+1} y\,.
\end{align*}
Continuing this way we get
\begin{align*}
\EE{x^* F_{t,i+1} y | \F_{t-j} }
= x^* (I-\alpha A_P)^j F_{t-j,i+1} y\,, \quad j=1,2,\dots,t-i\,.
\end{align*}
Specifically, for $j=t-i$ we get
\begin{align*}
\EE{x^* F_{t,i+1} y | \F_{i} }  = x^* (I-\alpha A_P)^{t-i} y\,.
\end{align*}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma}\label{noisecancel}
Let $t>i\ge 1$ and let $x\in \C^d$ be a $\F_{i-1}$-measurable random vector. Then,
$\E[x^* F_{t,i+1}\zeta_{i}]=0$.
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
By \Cref{lem:genunroll},
\begin{align*}
\EE{x^* F_{t,i+1} \zeta_i | \F_{i} }  = x^* (I-\alpha A_P)^{t-i} \zeta_i\,.
\end{align*}
Using the tower rule,
\begin{align*}
\EE{x^* F_{t,i+1} \zeta_i | \F_{i-1} }
= x^* (I-\alpha A_P)^{t-i}\EE{ \zeta_i | \F_{i-1} }= 0\,.
\end{align*}
\end{proof}

\begin{lemma}\label{lem:unroll}
For all $t>i\ge 0$, $\E \ip{e_i,F_{t,i+1} e_i}=\E\ip{e_i,(I-\alpha A_P)^{t-i} e_i}$.
\end{lemma}
\begin{proof}
The lemma follows directly from \Cref{lem:genunroll}. Indeed,
$\theta_i$ depends only on $A_1,\dots,A_{i},b_1,\dots,b_{i}$, $\theta_i$ and so is $e_i$ $\F_i$-measurable.
Hence, the lemma is applicable and implies that
\begin{align*}
\EE{ \ip{e_i, F_{t,i+1} e_i} | \F_i } =
\EE{ \ip{e_i, (I-\alpha A_P)^{t-i} e_i} | \F_i }\,.
\end{align*}
Taking expectation of both sides gives the desired result.
\end{proof}

\begin{lemma}\label{innerproduct}
Let $i>j \ge 0$ and let $x\in \R^d$ be an $\F_j$-measurable random vector.
Then,
\begin{align*}
\E\ip{F_{i,j+1}x,F_{i,j+1}x}\leq (1-\alpha \rhos{P})^{i-j}\E\norm{x}^2\,.
\end{align*}
\end{lemma}
\begin{proof}
Note that
$S_t\doteq \EE{ (I-\alpha A_t)^* (I-\alpha A_t) | \F_{t-1} }
= I - \alpha (A_P^* + A_P) + \alpha^2 \EE{ A_t^* A_t | \F_{t-1} }$.
Since $(b_t,A_t)_t$ is an independent sequence, $\EE{ A_t^* A_t|\F_{t-1}} = \EE{ A_1^* A_1 }$.
Now, using the definition of $\rhos{P}$ from \Cref{def:spect}
$\sup_{x:\norm{x}= 1} x^* S_t x = 1 - \alpha \inf_{x:\norm{x}=1} x^\top (A_P^* + A_P - \alpha \EE{A_1^\top A_1}) x
= 1-\alpha \rhos{P}$.
Hence,
\begin{align*}
&\EE{\ip{F_{i,j+1}x,F_{i,j+1}x}|\F_{i-1} }\\
&= \EE{x^* F_{i-1,j+1}^\top (I-\alpha A_i)^* (I-\alpha A_i) F_{i-1,j+1} x\,|\,\F_{i-1}}\\
&=(x F_{i-1,j+1})^* \, S_i \, F_{i-1,j+1} x\\
&\le (1-\alpha \rhos{P}) \, \ip{ F_{i-1,j+1} x, F_{i-1,j+1} x}
\end{align*}
We have $\EE{}=\EE{\cdot|\F_{i-1}|\F_{i-2}}=\ldots=\EE{\cdot|\F_{i-1}\ldots |\F_{j}}$. Now
\begin{align*}
&\EE{\ip{F_{i,j+1}x,F_{i,j+1}x}|\F_{i-1} |\F_{i-2}}\\
&\le (1-\alpha \rhos{P}) \, \EE{\ip{ F_{i-1,j+1} x, F_{i-1,j+1} x}|\F_{i-2}}
& \le (1-\alpha \rhos{P})^2\, \EE{\ip{ F_{i-2,j+1} x, F_{i-2,j+1} x} |\F_{i-3}}\\
& \quad \vdots \\
& \le (1-\alpha \rhos{P})^{i-j}\, \norm{x}^2\,.
%&= \E[x^\top (I-\alpha H_{j+1})\ldots (I-\alpha H_i)^\top (I-\alpha H_i)\ldots (I-\alpha H_{j+1})x|\F_{i-1}]\\
%&=(1-\alpha\rhos{P}) \E[x^\top (I-\alpha H_{j+1})\ldots (I-\alpha H_{i-1})^\top (I-\alpha H_{i-1})\ldots (I-\alpha H_{j+1})x]
\end{align*}
\end{proof}


\begin{theorem}\label{th:pdrate}
Let $\eh_t$ be as in \Cref{def:err}. %Let $P$ be a distribution over $\C^d\times\C^{\dcd}$ satisfying \Cref{assmp:lsacmplx}.
Then,
\begin{align}
%\EE{\norm{\eh_t}^2}
%\leq
%\left(1+\frac2{\alpha\rhod{P}}\right)\frac{1}{\alpha \rhos{P}} \left(\frac{\norm{e_0}^2}{(t+1)^2}+ \frac{\alpha^2{\sigma}_1^2+ {\sigma}_2^2 (\alpha^2\norm{\ts}+\alpha \norm{e_0})}{t+1} \right)\,.
\E[\norm{\eh_t}^2]
\leq \left(1+\frac2{\alpha\rhod{P}}\right)\, \frac1{\alpha\rhos{P}}\, \,
%\left(\frac{\norm{e_0}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_P^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_P^2\norm{\ts})\norm{e_0}}{t+1} \right)\,.
\left(\frac{\norm{e_0}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_1^2)+\alpha \sigma_2^2\norm{e_0}}{t+1} \right)\,.
\end{align}
\end{theorem}
\begin{proof}
By standard calculation using \eqref{eq:errrec} recursively, 
\begin{align*}
e_t&=(I-\alpha A_t)e_{t-1}+\alpha \zeta_t
& = (I-\alpha A_t) (I-\alpha A_{t-1}) e_{t-2}\\ &+ \alpha (I-\alpha A_t) \zeta_{t-1} +\alpha \zeta_t \\
& \quad \vdots\\
& = (I-\alpha A_t) \cdots (I-\alpha A_1) e_0\\ &+ \alpha (I-\alpha A_t) \cdots (I-\alpha A_2) \zeta_1 \\
& + \alpha (I-\alpha A_t) \cdots (I-\alpha A_3) \zeta_2\\
&  \quad \vdots \\
&+ \alpha \zeta_t\,,
\end{align*}
which can be written compactly as
\begin{align}
\label{eq:etft}
e_t = F_{t,1} e_0 + \alpha (F_{t,2} \zeta_1 + \dots + F_{t,t+1} \zeta_t )\,,
\end{align}
$\eh_t=\frac{1}{t+1}{\sum}_{i=0}^{t}e_i =\frac{1}{t+1}\Big\{{\sum}_{i=0}^{t} F_{i,1} e_0 + \alpha \sum_{i=1}^{t} \left(\sum_{k=i}^{t} F_{k,i+1} \right)\zeta_i \Big\} ,$
where in the second sum we flipped the order of sums and swapped the names of the variables that the sum runs over.
We not expand the MSE as below:
\begin{align*}
\E[\norm{\eh_t}^2]&=\E\ip{\eh_t,\eh_t}=\frac{1}{(t+1)^2} \sum_{i,j=0}^t \E\ip{e_i,e_j}= \frac{1}{(t+1)^2}\left(\sum_{i=0}^t \EE{\norm{e_i}^2}+2\sum_{i=0}^{t-1}\sum_{j>i} \re{\EE{\ip{e_i,e_j}}}\right)
\end{align*}
Hence, we see that it suffices to bound $\EE{\ip{ e_i,  e_j }}$.
There are two cases depending on whether $i=j$. When $j> i$,
\begin{align*}
\EE{\ip{e_i,e_j}}
&=\E \ip{e_i,\big[F_{j,i+1} e_i+\alpha\textstyle\sum_{k=i+1}^j F_{j,k+1}\zeta_{k}\big]}\\
&=\E\ip{e_i,F_{j,i+1} e_i}  \text{(from \Cref{noisecancel})}\\
&=\E\ip{e_i, (I-\alpha A)^{j-i} e_i} \text{(from \Cref{lem:unroll})}
\end{align*}
We now use the fact that for $z\in \C$, $\re{z}\leq \md{z}$ and hence $\re{\EE{\ip{e_i,e_j}}}\leq \md{\EE{\ip{e_i,e_j}}}$. Now,  $\md{\EE{\ip{e_i,e_j}}}\leq \md{\EE{\ip{e_i,(I-\alpha A)^{j-1}e_i}}}$, and from Cauchy-Schwatz we have $\md{\EE{\ip{e_i, (I-\alpha A)^{j-i}e_i}}}\leq \EE{\norm{e_i}^2}(1-\frac{\alpha\rhod{P}}{2})$.
\begin{align*}
\label{inter}
\sum_{i=0}^{t-1}\sum_{j=i+1}^t \md{\EE{\ip{e_i,e_j}}}
&=\sum_{i=0}^{t-1}\sum_{j=i+1}^t \left(1-\frac{\alpha\rhod{P}}{2}\right)^{j-i} \EE{\norm{e_i}^2}\\
&\leq \sum_{i=0}^{t-1} \EE{\norm{e_i}^2} \sum_{j=i+1}^\infty \left(1-\frac{\alpha\rhod{P}}{2}\right)^{j-i} \\
&\stackrel{(a)}{\leq} \sum_{i=0}^{t-1} \EE{\norm{e_i}^2} \frac{2}{\alpha \rhod{P}}\\
&=\frac2{\alpha\rhod{P}} {\sum}_{i=0}^{t-1}\E\ip{e_i,e_i}\\
&\leq \frac2{\alpha\rhod{P}}{\sum}_{i=0}^{t}\E\ip{e_i,e_i}\,.
\end{align*}
Since $\sum_{i,j}\cdot{} = \sum_{i=j}\cdot{} + 2 \sum_i \sum_{j>i} \re \cdot{}$,
\begin{align*}
{\sum}_{i=0}^{t}{\sum}_{j=0}^{t} \E\ip{e_i,e_j}&= \left(1+\frac4{\alpha\rhod{P}}\right){\sum}_{i=0}^{t}\E\ip{e_i,e_i}\,.
\end{align*}
Expanding $e_i$ using \eqref{eq:etft} and then using \Cref{innerproduct} and \Cref{assmp:lsacmplx}
\begin{align*}
\E\ip{e_i,e_i}&=\E\ip{F_{i,1}e_0,F_{i,1}e_0}+\alpha^2{\sum}_{j=1}^i\E\ip{ F_{i,j+1}\zeta_j, F_{i,j+1}\zeta_j}+\alpha\sum_{j=1}^i  \E\ip{F_{i,1} e_0, F_{i,j+1}\zeta_j}\\
&\leq (1-\alpha\rhos{P})^i\norm{e_0}^2+ \alpha^2\frac{{\sigma}_1^2}{\alpha \rhos{P}}+ \alpha \frac{{\sigma}^2_2 \norm{e_0}}{\alpha\rhos{P}}\,,
\end{align*}
and so
\begin{align*}
{\sum}_{i=0}^{t}{\sum}_{j=0}^{t} \E\ip{e_i,e_j}
&\leq \left(1+\frac4{\alpha\rhod{P} }\right)\, \frac1{\alpha\rhos{P}}\, (t(\alpha^2{\sigma}_1^2+\alpha {\sigma}^2_2\norm{e_0}) +\norm{e_0}^2)\,.
\end{align*}
Putting things together,
\begin{align}
\E[\norm{\eh_t}^2]
\leq \left(1+\frac4{\alpha\rhod{P}}\right)\, \frac1{\alpha\rhos{P}}\, \,
%\left(\frac{\norm{e_0}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_P^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_P^2\norm{\ts})\norm{e_0}}{t+1} \right)\,.
\left(\frac{\norm{e_0}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_1^2)+\alpha \sigma_2^2\norm{e_0}}{t+1} \right)\,.
\end{align}
\end{proof}


%\lmhur*
\todoc[inline]{Lemma text here.. got replaced.. work in progress..}
\begin{proof}
We know that $A_P$ is Hurwitz and from  \Cref{lm:simtran} it follows that there exists an $U\in \gld$ such that  $\Lambda=U^{-1} A_P U$ and $(\Lambda^*+\Lambda)$ is real symmetric and positive definite. Using \Cref{def:spect}, we have $A_{P_U}=\Lambda$ and from \Cref{lm:pd} we know that there exists an $\alpha_{P_U}$ such that $\rhod{P_U}>0$ and $\rhos{P_U}>0,~\forall \alpha \in (0,\alpha_{P_U})$.

\end{proof}



\begin{lemma}[Change of Basis]\label{lm:cb}
Let $P$ be a distribution over $\R^d\times \R^{\dcd}$ as in \Cref{assmp:lsa} and let $U$ be chosen according to \Cref{lm:hur}. Define $\gamma_t\eqdef U^{-1}\theta_t,\,,\gamma_*\eqdef U^{-1}\ts$, then
\begin{align}
\EE{\norm{\gamma_t-\gamma_*}^2}
\leq
\left(1+\frac2{\alpha\rhod{P_U}}\right)\frac{\norm{U^{-1}}^2}{\alpha \rhos{P_U}}
\left(\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_P^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_P^2\norm{\ts})\norm{\theta_0-\ts}}{t+1} \right)\,.
%\left(\frac{\norm{\gamma_0-\gamma_*}^2}{(t+1)^2}+ \frac{\alpha^2{\sigma}_1^2+ {\sigma}_2^2 (\alpha^2+\alpha \norm{\gamma_0-\gamma_*})}{t+1} \right)\,,
\end{align}
where $\gh_t=\frac{1}{t+1}\sum_{s=0}^t \gamma_s$.
\end{lemma}
\begin{proof}
Consider the modified error recursion in terms of $z_t\eqdef \gamma_t-\gamma_*$
\begin{align}\label{eq:newerrrec}
\begin{split}
e_t&=(I-\alpha A_t)e_{t-1}+\alpha\zeta_t\\
U^{-1}e_t&=(I-\alpha U^{-1}A_t U) U^{-1}e_{t-1}+ \alpha U^{-1}\zeta_t\\
z_t&=(I-\alpha \Lambda_t) z_{t-1}+\alpha H_t,
\end{split}
\end{align}
where  $\Lambda_t=U^{-1}A_t U$ and $H_t=U^{-1}\zeta_t$. Note that the error recursion in $z_t$ might involve complex computations (depending on whether $U$ has complex entries or not), and hence \eqref{eq:lsacmplx} and \Cref{assmp:lsacmplx} are useful in analyzing $z_t$.
We know that $\EE{\norm{H_t}^2}\leq \norm{U^{-1}}^2\EE{\norm{\zeta_t}}$ and $\EE{\norm{\Lambda_t H_t}}=\EE{\norm{U^{-1}A_t UU^{-1}\zeta_t}}=\EE{\norm{U^{-1}A_t \zeta_t}}\leq \norm{U^{-1}}\EE{\norm{A_t\zeta_t}}=\norm{U^{-1}}\sigma_2^2$. Now applying \Cref{th:pdrate} to $\hat{z}_t\eqdef \frac{1}{t+1}\sum_{s=0}^t z_t$, we have
\begin{align}
\E[\norm{\zh_t}^2]
&\leq \left(1+\frac2{\alpha\rhod{P_U}}\right)\, \frac1{\alpha\rhos{P_U}}\, \,
\left(\frac{\norm{z_0}^2}{(t+1)^2}+ \frac{\alpha^2(\norm{U^{-1}}^2\sigma_1^2)+\alpha (\norm{U^{-1}}\sigma_2^2)\norm{z_0}}{t+1} \right)\,\\
&\leq \left(1+\frac2{\alpha\rhod{P_U}}\right)\, \frac1{\alpha\rhos{P_U}}\, \,
\left(\frac{\norm{U^{-1}}^2\norm{e_0}^2}{(t+1)^2}+ \frac{\alpha^2(\norm{U^{-1}}^2\sigma_1^2)+\alpha (\norm{U^{-1}}\sigma_2^2)\norm{U^{-1}}\norm{e_0}}{t+1} \right)\,
\end{align}

\end{proof}

\thrate*
\begin{proof}
Follows by substituting $\theta_t=U\gamma_t$ in \Cref{lm:cb}.
\end{proof}
\begin{comment}
\begin{theorem}[Lower Bound]
There exists a distribution $P$ over $\R^d\times \R^{\dcd}$ satisfying \Cref{assmp:lsa}, such that $\alpha_P>0$, such that $\rhos{P}>0$ and $\rhod{P}>0,\,\forall \alpha\in (0,\alpha_P)$ and
\begin{align*}
\EE{\norm{\thh_t-\ts}^2} &\geq \frac{1}{(t+1)^2}(\alpha)^{-2}(\rhod{P}\rhos{P})^{-1}\Big( \big(1-(1-\alpha \rhos{P})^t\big) \norm{\theta_0-\ts}^2 \\&+ \sum_{s=1}^t \big(1-(1-\alpha \rhos{P})^{(t-s)}\big) \big(\alpha^2(\sigma_A^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_A^2\norm{\ts})\norm{\theta_0-\ts}\big) \Big)
\end{align*}
\end{theorem}
\end{comment}
\thlb*
\begin{proof}
Consider the LSA with $(b_t,A_t)\sim P$ such that $b_t=(N_t,0)^\top\in\R^2$ is a zero mean \iid random variable with variance $\sigma^2_b$, and $A_t=A,\,\forall t\geq 0$, where $A=A_P=\begin{bmatrix} \lambda_{\min} &0\\ 0& \lambda_{\max}\end{bmatrix}$, for some $\lambda_{\max}>\lambda_{\min}>0$. Note that in this example $\ts=0$.
By choosing $\alpha<\frac2{\lambda_{\max}}$, in this case it is straightforward to write the expression for $\eh_t$ explicitly as below:
\begin{align*}
\eh_t&=\frac{1}{t+1}\sum_{s=0}^t e_t = \frac{1}{t+1}\sum_{s=0}^t (I-\alpha A_P)^{t-s} e_0 + \sum_{s=1}^t \sum_{i=s}^t (I-\alpha A_P)^{i-s} b_s\\
&=\frac{1}{t+1}(\alpha A_P)^{-1}\left[\left(I-(I-\alpha A_P)^{t+1}\right)e_0 + \sum_{s=1}^t \left(I-(I-\alpha A_P)^{t+1-s}\right) b_s\right]\,.
\end{align*}
Thus,
\begin{align*}
\EE{\norm{\eh_t}^2}&\stackrel{(a)}{=}\frac{1}{(t+1)^2}\Big[\norm{(\alpha A_P)^{-1}\left(I-(I-\alpha A_P)^{t+1}\right)e_0}^2 \\ 
&+\sum_{s=1}^t \norm{(\alpha A_P)^{-1}\left(I-(I-\alpha A_P)^{t+1-s}\right)b_s}^2\Big]\,,
%&\leq\frac{1}{(t+1)^2}\alpha^2\norm{A_P^{-1}}^2\norm{e_0}^2
\end{align*}
and hence
\begin{align*}
\EE{\norm{\eh_t}^2}
& \geq \EE{\eh^2_t(1)}\stackrel{(b)}{=}\frac{1}{(t+1)^2}(\alpha \lambda_{\min})^{-2}\Big[\left(1-(1-\alpha \lambda_{\min})^{t+1}\right)^2 \theta^2_0(1)\\
& + \frac{1}{(t+1)^2}\sum_{s=1}^t\left(1-(1-\alpha \lambda_{\min})^{t+1-s}\right)^2 b^2_s(1) \Big]\,.
\end{align*}
Here $(a)$ and $(b)$ follows from the \iid assumption. Note that in this example, $\rhos{P}=\rhod{P}=2\lambda_{\min} -\alpha \lambda_{\min}^2=\lambda_{\min}(2-\alpha \lambda_{\min})$, and $\norm{\ts}=0$ and $\sigma^2_A=0$. Further, the result follows by noting the fact that noting the fact that $\norm{b_t}^2=b_t(1)^2$ and $\norm{\theta_t}^2=\theta_t(1)^2$.
%\begin{align}
%\EE{\norm{\eh_t}^2}\geq \frac{1}{(t+1)^2}(\alpha \lambda_{\min})^{-2}(1-\alpha \lambda_{\min}^t) \theta^2_0(1)
%\end{align}
\end{proof}
\begin{comment}
\textbf{Proof of }
Given any $A_P$ we can choose a unitary matrix $U$ (i.e., $U^\top U=I$) such that $U^\top A_P U=D_P$, where $D_P$ is a diagonal matrix. Thus with $\gamma=U\top \theta$, we have
\begin{align}
\theta_{t}=(I-\alpha A_P)\theta_{t-1}+\alpha b_t\\
\label{eq:gamrec} \gamma_t=(I-\alpha D_P)\gamma_{t-1}+\alpha \zeta_t,
\end{align}
where $\zeta_t=U^\top b_t$. Note that the unitary transformation preserves norm, i.e., $\norm{\gamma}=\theta^\top UU^\top \theta=\norm{\theta}$, $\norm{\theta}_A=\norm{\gamma}_{\Lambda}$ and $\EE{\zeta_t\zeta_t^\top}=\EE{U^\top b_t b_t^\top U}\leq D_P$. Since, $\Lambda_P$ is a diagonal, \eqref{eq:gamrec} has $d$ separate $1$-dimensional equations. In what follows, without loss of generality, all quantities are in $1$-dimension. Also, $\ts=\gamma_*=\mathbf{0}$. Also, let $e_0=\gamma_0$
\begin{align}
\eh_t=\frac{1}{t+1}(\alpha \Lambda)^{-1}\big([I-(I-\Lambda D)^{t+1}]e_0
+\alpha\sum_{s=1}^t [I-(I-\alpha \Lambda)^{t+1-s}] \zeta_s]\big)
\end{align}
Now for small $t$ such that $\alpha \Lambda t<I$,  we have $I- (I-\alpha \Lambda)^t\approx \alpha \Lambda t$. Thus, we have 
\begin{align}
\eh_t \approx \frac{1}{t+1}\big((t+1)e_0
+\alpha\sum_{s=1}^t (t+1-s) \zeta_s]\big)
\end{align}
Taking expectation, we have
\begin{align}
\EE{\norm{\eh_t}}&\approx \frac{1}{(t+1)^2} \big(\B (t+1)^2+\alpha^2 \sum_{s=1}^{t} s^2 \sigma^2_{b_P}),
%\EE{\norm{\eh_t}_\Lambda}&\approx \frac{1}{(t+1)^2} \big( (t+1)e_0^\top \Lambda e_0 +\sum_{s=1}^{t} s^2 \zeta_t^\top \Lambda \zeta_t)\\
\end{align}
The $\sum_{s=1}^t s^2=O(t^3)$, and hence we have $\EE{\norm{\eh_t}}\approx \frac{1}{(t+1)^2} \big(\B (t+1)^2+\alpha^2 O(t^3) \sigma^2_{b_P})$. 
Similarly, we have

$\EE{\norm{\eh_t}_\Lambda}\approx \frac{1}{(t+1)^2} \big((t+1)^2 \B \Lambda  +\alpha^2 O(t^3) \sigma^2_{b_P})$. Now, using $\alpha \Lambda t <1$, it follows that $\EE{\norm{\eh_t}_\Lambda}\approx \frac{1}{(t+1)^2} \big((t+1)^2\B \Lambda  +\alpha^2 O(t^3) \sigma^2_{b_P} \Lambda)\approx O(\frac{1}{\alpha (t+1)}+\alpha \sigma^2_{b_P})$. Further, in the case of $\P_{SPDSN}$, it follows $\sigma^2_{b_P}<\Lambda$, we have $\EE{\norm{\eh_t}_\Lambda}\approx \frac{1}{(t+1)^2} \big((t+1)^2\B \Lambda  +\alpha^2 O(t^3) \sigma^2_{b_P} \Lambda)\approx O(\frac{1}{\alpha (t+1)}+\frac{1}{t})$.

\end{comment}
\thpspd*
\begin{proof}
For any $P$ in $\P_{SPD}$ or $\P_{SPDSN}$, the corresponding $A_P$ matrix is real symmetric and positive definite. Thus, for each problem instance there exists an orthogonal matrix $U$ (i.e., $U^\top U=I$) such that $U^\top U=I$ and $U^\top A_P U=\Lambda_P$. Define $\zeta_t\eqdef U^\top b_t$, and $\gamma=U^\top \theta$. Now, we have
\begin{align}
\theta_t&=(I-\alpha A_P)\theta_{t-1}+\alpha b_t\\
\label{eq:gamrec}\gamma_t&=(I-\alpha \Lambda_P)\gamma_{t-1}+\alpha \zeta_t
\end{align}
Since $U$ is an orthogonal matrix $\norm{\gamma}_{\Lambda_P}=\gamma^\top \Lambda_P \gamma= \theta^\top U \Lambda_P U^\top \theta=\norm{\theta}_{A_P}$ and similarly $\norm{gamma}=\norm{\theta}$ and when $P\in \P_{SPDSN}$ we have $\EE{\zeta_t \zeta_t^\top}\leq \Lambda_P$. Thus, for any adversarial choice of $A_P$, it is equivalent to choosing a problem corresponding to $\Lambda_P$. Thus, in order to prove the bounds it is enough to choose such diagonal problems. In what follows, we let $e_t\eqdef\gamma_t-\gamma_*$ (though we have used $e_t=\theta_t-\ts$ everywhere else, we have re-defined to avoid introducing additional notation).

It is clear that \eqref{eq:gamrec}, has $d$ separate $1$-dimensional equations. Further, the MSE is a summation of $d$ separate terms (i.e. $\EE{\norm{\eh_t}}=\sum_{i=1}^d \eh_t(i)^2$, $\EE{\norm{\eh_t}_{\Lambda_P}}=\sum_{i=1}^d \Lambda(i) \eh_t(i)^2$, where $\Lambda(i),i=1,\ldots, d$ are the eigenvalues) and any adversarial choice will maximize each of the $d$ terms. Thus, it follows that such an adversarial choice should have $\Lambda(i)=\Lambda, i=1,\ldots, d$. Thus, it is enough to prove the bounds for the $1$-dimensional case, and multiply them with $d$ for the general case. In what follows, we deal with the $1$-dimensional case, where $\lambda\in (0,1)$ is the eigenvalue, and $b_t\in \R$ is the noise sequence and the condition $\EE{b_tb_t^\top}\leq A_P$ translates to $\EE{b_t^2}\leq \lambda$. In the $1$-dimensional case, we have the following relation for the error
\begin{align}\label{eq:onederr}
\eh_t=\frac{1}{t+1}\left[\sum_{s=0}^t (1-\alpha \lambda)^s e_0+ \alpha \sum_{i=1}^t \sum_{s=0}^{t-i} (1-\alpha \lambda)^s b_i\right]
\end{align}
We now look at cases $(i)$, $(ii)$ and $(iii)$, and prove matching lower and upper bounds. In what follows, we use the facts
\begin{enumerate}
\item For sufficiently large $t$ we have $(1-\frac{1}{t+1})^t\geq c$ (the constant $c\approx \frac{1}{e}$, where $e$ is the base of the natural logarithm). This will be used in the lower bound proofs.
\item $\sum_{s=0}^t(1-\alpha \lambda)^2\leq \min\{(\alpha\lambda)^{-1},(t+1)\}$. This will be used in the upper bound proofs.
 \end{enumerate}
\textbf{$(i)$ Lower Bound:}
 For sufficiently large $t$, and any step-size$\alpha \in (0,1)$ pick $\lambda$ such that $\alpha \lambda (t+1)=1$. Taking expectation in \eqref{eq:onederr}, and letting $\B=e_0^2$, we have
\begin{align}\label{eq:intererr}
\EE{\norm{\eh_t}^2}=\frac{1}{(t+1)^2}\left[ (\sum_{s=0}^t(1-\alpha \lambda)^s)^2 \B+ \alpha^2\sigma^2_{b_P} \sum_{i=1}^t (\sum_{s=0}^{t-i} (1-\alpha \lambda)^s)^2\right],
\end{align}
where we use the property of independence. Now for this choice of $\lambda$ we have
\begin{align}
\EE{\norm{\eh_t}^2}&\geq \frac{1}{(t+1)^2}\left[ c^2(t+1)^2 \B+ \alpha^2\sigma^2_{b_P} c^2\sum_{i=1}^t (i)^2\right]\\
&\geq c^2\left[B+ \alpha^2 \sigma^2_{b_P} \Theta(t) \right],
\end{align}
which completes the lower bound part for this case.

\textbf{$(i)$ Upper Bound:}
Starting from \eqref{eq:intererr}, we have
\begin{align}
\EE{\norm{\eh_t}^2}&\leq \frac{1}{(t+1)^2}\left[\B (\min\{(\alpha\lambda)^{-1},(t+1)\})^2+ \alpha^2 \sigma^2_{b_P} (t+1) (\min\{(\alpha\lambda)^{-1},(t+1)\})^2 \right]\\
&= \left[\B (\min\{\frac{1}{\alpha \lambda (t+1)},1)^2+ \alpha^2 \sigma^2_{b_P} (t+1) (\min\{\frac{1}{\alpha \lambda (t+1)},1)^2 \right]\\
\leq \left[\B + \alpha^2 \sigma^2_{b_P} (t+1) \right], 
\end{align}
which completes the upper bound part for this case.

\textbf{$(ii)$ Lower Bound:}
For sufficiently large $t$, and any step-size $\alpha \in (0,1)$ pick $\lambda$ such that $\alpha \lambda (t+1)=1$. Taking expectation in \eqref{eq:onederr}, and letting $\B=e_0^2$, we have
\begin{align}\label{eq:intererr}
\EE{\norm{\eh_t}^2_{\lambda}}=\frac{1}{(t+1)^2}\left[ \lambda (\sum_{s=0}^t(1-\alpha \lambda)^s)^2 \B+ \alpha^2\sigma^2_{b_P}\lambda \sum_{i=1}^t (\sum_{s=0}^{t-i} (1-\alpha \lambda)^s)^2\right],
\end{align}
where we use the property of independence. Now for this choice of $\lambda$ we have
\begin{align}
\EE{\norm{\eh_t}^2}&\geq \frac{1}{(t+1)^2}\left[ \lambda c^2(t+1)^2 \B+ \lambda \alpha^2\sigma^2_{b_P} c^2\sum_{i=1}^t (i)^2\right]\\
&= c^2\left[\frac{\B}{\alpha (t+1)}+ \alpha  \sigma^2_{b_P} \Theta(1) \right],
\end{align}
which completes the lower bound part for this case.

\textbf{$(ii)$ Upper Bound:}
Starting from \eqref{eq:intererr}, we have
\begin{align}
\EE{\norm{\eh_t}^2_{\lambda}}&\leq \frac{1}{(t+1)^2}\left[\lambda \B (\min\{(\alpha\lambda)^{-1},(t+1)\})^2+ \lambda \alpha^2 \sigma^2_{b_P} (t+1) (\min\{(\alpha\lambda)^{-1},(t+1)\})^2 \right]\\
&= \left[\B (\min\{\frac{1}{\alpha \lambda (t+1)},1)^2+ \lambda \alpha^2 \sigma^2_{b_P} (t+1) (\min\{\frac{1}{\alpha \lambda (t+1)},1)^2 \right]\\
\end{align}
Here if $\min{(\alpha\lambda)^{-1},(t+1)}=\frac{1}{\alpha \lambda}$, then we have $\lambda(\min{(\alpha\lambda)^{-1},(t+1)})^2=\frac{1}{\alpha^2 \lambda}\leq \frac{(t+1)}{\alpha}$, and hence $\frac{1}{(t+1)^2}\lambda(\min{(\alpha\lambda)^{-1},(t+1)})^2=\frac{1}{\alpha (t+1)}$. In the other case, when $\min{(\alpha\lambda)^{-1},(t+1)}=(t+1)$, we have
$\frac{1}{(t+1)^2}\lambda(\min{(\alpha\lambda)^{-1},(t+1)})^2=\frac{1}{\alpha (t+1)}$. Thus
\begin{align}
\EE{\norm{\eh_t}^2_{\lambda}}&\leq \left[\B \frac{1}{\alpha (t+1)}+ \alpha \sigma^2_{b_P} \right]\\
\end{align}
This completes the upper bound part for this case.

\textbf{$(iii)$ Lower Bound:}
For sufficiently large $t$, and any step-size$\alpha \in (0,1)$ pick $\lambda$ such that $\alpha \lambda (t+1)=1$. Taking expectation in \eqref{eq:onederr}, and letting $\B=e_0^2$, we have
\begin{align}\label{eq:intererr}
\EE{\norm{\eh_t}^2_{\lambda}}=\frac{1}{(t+1)^2}\left[ \lambda (\sum_{s=0}^t(1-\alpha \lambda)^s)^2 \B+ \alpha^2\sigma^2_{b_P}\lambda \sum_{i=1}^t (\sum_{s=0}^{t-i} (1-\alpha \lambda)^s)^2\right],
\end{align}
where we use the property of independence. Now for this choice of $\lambda$ we have
\begin{align}
\EE{\norm{\eh_t}^2}&\geq \frac{1}{(t+1)^2}\left[ \lambda c^2(t+1)^2 \B+ \lambda \alpha^2 \lambda c^2\sum_{i=1}^t (i)^2\right]\\
&= c^2\left[\frac{\B}{\alpha (t+1)}+   \Theta(\frac{1}{t}) \right],
\end{align}
where we use $\sigma^2_{b_P}=\lambda$. This completes the lower bound part for this case.

\textbf{$(iii)$ Upper Bound:}
Starting from \eqref{eq:intererr}, we have
\begin{align}
\EE{\norm{\eh_t}^2_{\lambda}}&\leq \frac{1}{(t+1)^2}\left[\lambda \B (\min\{(\alpha\lambda)^{-1},(t+1)\})^2+ \lambda \alpha^2 \sigma^2_{b_P} (t+1) (\min\{(\alpha\lambda)^{-1},(t+1)\})^2 \right]\\
&= \left[\B (\min\{\frac{1}{\alpha \lambda (t+1)},1)^2+ \lambda \alpha^2 \lambda (t+1) (\min\{\frac{1}{\alpha \lambda (t+1)},1)^2 \right]\\
\end{align}
Here if $\min{(\alpha\lambda)^{-1},(t+1)}=\frac{1}{\alpha \lambda}$, then we have $\lambda(\min{(\alpha\lambda)^{-1},(t+1)})^2=\frac{1}{\alpha^2 \lambda}\leq \frac{(t+1)}{\alpha}$, and hence $\frac{1}{(t+1)^2}\lambda(\min{(\alpha\lambda)^{-1},(t+1)})^2=\frac{1}{\alpha (t+1)}$. In the other case, when $\min{(\alpha\lambda)^{-1},(t+1)}=(t+1)$, we have
$\frac{1}{(t+1)^2}\lambda(\min{(\alpha\lambda)^{-1},(t+1)})^2=\frac{1}{\alpha (t+1)}$. Thus
\begin{align}
\EE{\norm{\eh_t}^2_{\lambda}}&\leq \left[\B \frac{1}{\alpha (t+1)}+ \alpha \frac{1}{\alpha (t+1)} \right]\\
\end{align}
where we use $\sigma^2_{b_P}=\lambda$. This completes the upper bound part for this case.
\end{proof}
\begin{comment}

It is trivial to check that any $\alpha \in (0,1)$ is a universal step-size for $\P_{USN}$ and $\P_{SN}$. Now for a given $t$, pick $A_P\in (0,1)$ such that $\alpha A_P t=1$. Also, note that $\ts=0$ for any problem in $\P_{USN}$ and $\P_{SN}$. Now, let $\eh_t\eqdef \thh_t-\ts$, we have

\begin{align}
\eh_t=\frac{1}{t+1}(\alpha A_P)^{-1}\big([I-(I-\alpha A_P)^{t+1}]e_0
+\alpha\sum_{s=1}^t [I-(I-\alpha A_P)^{t+1-s}] b_s]\big)
\end{align}
 We have for $t\geq 0$ such that $\alpha A_P t\leq 1$,  $I- (I-\alpha A_P)^t\approx(\alpha A_P t)$. Thus, we have 
\begin{align}
\eh_t \approx\left(\frac{1}{t+1}[ (t+1)e_0
+\alpha\sum_{s=1}^t (t+1-s)b_s]\right)
\end{align}
Taking expectation, we have with $\B=\norm{e_0}^2$
\begin{align}
\EE{\norm{\eh_t}}\approx\left( \frac{1}{(t+1)^2} [\B (t+1)^2+\alpha^2 \sum_{s=1}^{t} s^2 \sigma^2_{b_P}]\right),
\end{align}
The $\sum_{s=1}^t s^2= O(t^3)$, and hence we have $\EE{\norm{\eh_t}}\approx\left(\frac{1}{(t+1)^2} [\B (t+1)^2+\alpha^2 O(t^3) \sigma^2_{b_P}]\right)$. Similarly, we have 
\begin{align}
\EE{\norm{\eh_t}_{A_P}}\approx\left( \frac{1}{(t+1)^2} [(t+1)^2 \B A_P  +\alpha^2 O(t^3) \sigma^2_{b_P}]\right)
\end{align}
Now, using $\alpha A_P t <1$, it follows that 
\begin{align}
\EE{\norm{\eh_t}_{A_P}}& \approx\left(\frac{1}{(t+1)^2} [(t+1)^2\B A_P  +\alpha^2 O(t^3) \sigma^2_{b_P} A_P]\right) \\
&\approx\left(\frac{1}{\alpha (t+1)}+\alpha \sigma^2_{b_P}\right)
\end{align}
Further, in the case of $\P_{SPDSN}$, it follows $\sigma^2_{b_P}<A_P$, we have 
\begin{align}
\EE{\norm{\eh_t}_{A_P}}&=\approx \left(\frac{1}{(t+1)^2} [(t+1)^2\B \Lambda  +\alpha^2 O(t^3) \sigma^2_{b_P} A_P] \right)\\ 
&\approx\left(\frac{1}{\alpha (t+1)}+\frac{1}{t}\right)
\end{align}
\end{comment}

 \propunistep*

\begin{proof}
The eigenvalues of $I-\alpha A_P$ are $1-\alpha u\pm i v$, and we can ensure that their modulus is less than unity only when $\alpha<\frac{u}{\sqrt{u^2+v^2}}$. By, letting $u\ra 0$ we can make $\alpha\ra 0$.
\end{proof}

\begin{lemma}\label{lm:matnorm}
Let $A$ be a $\dcd$ matrix with $B\eqdef\max_{ij}\left|A_{ij}\right|$. It follows that $\max_{x\in \R^d: \norm{x}\leq1}x^\top A^\top A x <B^2d$.
\end{lemma}

We now resort to an the following notation for real symmetric positive definite $\dcd$ matrices $C$ and $D$: $C\succ D$ if $C-D$ is positive definite.
\begin{lemma}\label{lm:schur}
Let $A,B,C$ $\dcd$ real matrices. Given a symmetric matrix $M=\left[\begin{matrix}A&B \\B^\top &C\end{matrix}\right]$ be a given $2d\times 2d$ matrix, it follows that $M\succ 0$ if	$A\succ$ and $C-B^\top A^{-1}B\succ 0$.
\end{lemma}

\begin{lemma}\label{lm:amat}
Let $\delta\in(0,1)$ a given discount factor and let $\pi$ be a policy. It follows that the matrix $A_\delta=\Phi^\top D_\pi(I-\delta P_\pi)\Phi$ is positive definite ($x^\top Ax>0,\forall x \in \R^d$) $\forall \delta\in(0,1)$.
\end{lemma}

\thtdadmis*
\begin{proof}
We need to show for the said values of $\alpha$ it holds that $\rhos{P}>0$. %This is equivalent to showing $\max_{x\in \R^d: \norm{x}\leq 1}\E{x^\top(I-\alpha A_t)^\top (I-\alpha A_t)x}<1$. 

\textbf{$(i)$}   We have $\alpha\phi^\top\phi=\frac{1}{B^2d}\phi^\top_t\phi_t<\beta$ for some $\beta\in (0,1)$. Now, 
\begin{align*}
(A_t+A_t^\top) -\alpha A_t^\top A_t =& 2\phi_t(\phi_t-\gamma\phi'_t)^\top -\beta\phi_t(\phi-\gamma\phi'_t)^\top+\beta\gamma\phi'_t(\phi_t-\gamma{\phi'}_t)^\top\\
&= (2-\beta)\phi_t(\phi_t-\gamma\phi'_t)^\top +\beta\gamma\phi'_t\phi^\top_t-\beta\gamma^2\phi'_t{\phi'}_t^\top \\
&= (2-\beta)\phi_t(\phi_t-\gamma\phi'_t)^\top -\beta\gamma^2(\gamma\phi'_t\phi^\top_t- \phi'_t{\phi'}_t^\top)-\beta\gamma(1-\gamma^2)\phi'_t\phi_t^\top\\
\end{align*}
Taking $\E$, we have
\begin{align*}
&\EE{ (A_t+A_t^\top)-\alpha A_t^\top A_t}\\
&\succ  A -\beta\gamma^2(A)+\beta\gamma(1-\gamma^2)\E{\phi'_t\phi_t^\top} \big)x\\
&\succ (1-\gamma^2)[A +\beta\gamma\E{\phi'_t\phi_t^\top}]
\end{align*}
Now we have 
\begin{align*}
A +\beta\gamma\E{\phi'_t\phi_t^\top}=& \EE{\phi_t\phi_t}-\gamma\E{\phi_t{\phi'_t}^\top} +\beta\gamma\E{\phi'_t\phi_t^\top}\\
&=\E{\phi_t\phi_t}-\delta\E{\phi_t{\phi'_t}^\top},
\end{align*}
where $\delta=\gamma(1-\beta)$. However it follows that $x^\top\big(\E{\phi_t\phi_t}-\delta\E{\phi_t\phi'_t} )x=x^\top A_{\delta} x>0$ from \Cref{lm:amat}.

\textbf{$(ii)$:}
In the case of normalized features we have
\begin{align*}
(A_t+A_t^\top) -\alpha A_t^\top A_t &= 2\phi_t(\phi_t-\gamma\phi'_t)^\top - (\phi-\gamma\phi'_t)(\phi-\gamma\phi'_t)^\top\\
&=(\phi_t+\gamma\phi'_t)(\phi-\gamma\phi'_t)^\top\\
&=\phi_t\phi_t^\top-\gamma^2\phi'_t{\phi'_t}^\top
\end{align*}
Taking $\E$, we have
\begin{align*}
\EE{(A_t+A_t^\top) -\alpha A_t^\top A_t }&=\EE{\phi_t\phi_t^\top-\gamma^2\phi'_t{\phi'_t}^\top}\\
&=(1-\gamma^2)\EE{phi_t\phi_t^\top}
&\succ 0
\end{align*}

\textbf{$(iii)$}
 The GTD updates in \Cref{tb:tdalgos} can be expressed as $x_{t+1}=x_t+\alpha (g_t -H_t x_t)$, where $x_t=\left[\begin{matrix}y_t\\\theta_t\end{matrix}\right]$, $H_t=\left[\begin{matrix}I &A_t \\ -(1-\alpha)A^\top_t & \alpha A_t^\top A_t\end{matrix}\right]$, and $g_t=\left[\begin{matrix} b_t\\ A_t^\top b_t\end{matrix}\right]$, where $A_t=\phi_t(\phi_t-\gamma{\phi'}_t)^\top$. To show that $\alpha=\frac{1}{2B^4 d^2}$ is a universal step-size choice, we need to show that   $\inf_{x\in \R^d:\norm{x}=1}\E{x^\top[(H_t+H_t^\top) -\alpha H_t^\top H_t]x}>0$.

Now $H_t+H_t^\top=\left[\begin{matrix} 2I & \alpha A_t\\ \alpha A_t^\top 2\alpha A_t^\top A_t\end{matrix}\right]$ and $H_t^\top H_t=\left[\begin{matrix} I+(1-\alpha)^2A_tA_t^\top & A_t-\alpha(1-\alpha)A_tA_t^\top A_t\\ A_t^\top-\alpha(1-\alpha)A_t^\top A_t A_t^\top & A_t^\top A_t+\alpha^2A_t^\top A_t A_t^\top A_t\end{matrix}\right]$.  We see that $(H_t+H_t^\top) -\alpha(H_t^\top H_t) \succ M_t$, where $M_t=\left[\begin{matrix}I &\alpha^2(1-\alpha) A_tA_t^\top A_t\\ \alpha^2(1-\alpha) A_t^\top A_t A_t^\top &\alpha A_t^\top A_t-\alpha^3A_t^\top A_tA_t^\top A_t\end{matrix}\right]$. Now from \Cref{lm:schur}, to show that $M_t\succ 0$ we need to ensure 
\begin{align}
&\alpha A_t^\top A_t -\alpha^3A_t^\top A_t A_t^\top A_t - \alpha^4(1-\alpha)^2A_t^\top A_t A_t^\top A_t A_t^\top A_t\\
=&\alpha A_t^\top\big(I-\alpha^2 A_tA_t^\top-\alpha^3A_tA_t^\top A_tA_t^\top\big)A_t\\
\succ &\alpha A_t^\top\big(I-\alpha^2 A_t(I+\alpha A_t^\top A_t)A_t^\top\big)A_t\\
\succ& \frac{\alpha}{2} A_t^\top A_t,
\end{align}
where the last inequality follows from the choice of $\alpha$, bound $B$ on the features and \Cref{lm:matnorm}
\end{proof}
%\left[\begin{matrix}\end{matrix}\right]