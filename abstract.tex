%!TEX root =  flsa.tex
\begin{abstract}
\if
Stochastic approximation algorithms, 
such as the LMS algorithm in linear least squares regression (LS),
or TD(0) and GTD in reinforcement learning (RL),
are not only easy to implement, but their storage and iteration costs are also minimal.
Recent work has shown that for bounded LS,
the constant step-size version of the LMS algorithm with iterate averaging is also information-efficient, making it
an ideal choice for LS.
In this paper we investigate to what extent this result extends beyond LS.
Noticing that all the above methods have an update rule that takes a linear form,
\fi
In this paper we study study constant stepsize averaged linear stochastic approximation.
With an eye towards linear value estimation in reinforcement learning, 
we ask whether for a given class of linear estimation problems 
 $i)$ a single \emph{universal} constant stepsize with $ii)$ a $C/t$ worst-case expected error
 with a class-dependent constant $C>0$ can be guaranteed when the error is measured via 
 an appropriate weighted squared norm. Such a result has recently been obtained in the context
 of linear least squares regression.
We give examples that show that the answer to these questions in general is \emph{no}. 
On the positive side, we also characterize the instance dependent behavior of the error of the said algorithms,
identify some conditions under which the answer to the above questions can be changed to the positive,
and in particular show instance-dependent 
error bounds of magnitude $O(1/t)$ for the constant stepsize iterate averaged versions
of TD(0) and a novel variant of GTD, where the stepsize is chosen independently of the value estimation
instance. The result for TD(0) is shown when a so-called
second-order feature stationarity condition is satisfies, which can be viewed
as a replacement of the usual on-policy sampling constraint.
Computer simulations are used to illustrate and complement the theory.
\end{abstract}
