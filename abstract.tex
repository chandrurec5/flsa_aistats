\begin{abstract}
 We consider $d$-dimensional linear stochastic approximation algorithms (LSAs) with a constant step-size and the so called Polyak-Ruppert (PR) averaging (CS-PR in short) of iterates. Given a LSA with CS-PR, and for a problem $P$ from class $\P$,  we show $i)$ there exists a constant step-size $\alpha_P>0$ for which the mean-square error converges to zero at a rate of $O(\frac{C_P}{t})$, and in general the problem dependence (due to $P$ in $\alpha_P$ and $C_P$) cannot be avoided. We are also interested in the temporal difference class of LSA algorithms arising in reinforcement learning to solve the problem of approximate policy evaluation from experience replay. Here, we show that for the TD(0) algorithm universal step-size choice exists for $i)$ class of \emph{on-policy} problems with bounded features and $ii)$ the class of \emph{off-policy} problems with normalized features. We also introduce a modified version of GTD algorithm for which universal step-size choice exists for class of problems with bounded features. We also present numerical experiments that illustrate the theory.
\end{abstract}
