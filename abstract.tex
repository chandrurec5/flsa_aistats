\begin{abstract}
We consider $d$-dimensional \emph{linear inversion} problems (arising in machine learning) where the aim is to compute a $\ts\in \R^d$ such that $\ts=A^{-1}b$ using noisy samples of $A\in\R^{\dcd}$ and $b\in\R^d$. Linear stochastic approximation (LSA) is a widely used approach to solve such problems: the stochastic gradient descent and the temporal difference class of learning algorithms (such as TD(0) or GTD) for approximate value function estimation in reinforcement learning are LSA algorithms. An important parameter that affects the performance of LSA is the step-size or learning rate. In this paper,  we look at a constant step-size averaged linear stochastic approximation approach (CALSA) to solve the linear inversion problem, ask whether a uniformly fast rate of $O(\frac{1}{t})$ is achievable for the mean square-error across all problem instances. We show that the answer to this question, in general, is \emph{no}. However, we show that instance dependent  finite-time rate of $O(\frac{1}{t})$ is achievable and for some interesting problem classes in reinforcement learning the constant step-size can be chosen in a problem independent manner.
 \end{abstract}

\begin{comment}
 We consider $d$-dimensional linear stochastic approximation algorithms (LSAs) with a constant step-size and the so called Polyak-Ruppert (PR) averaging (CS-PR in short) of iterates. Given a LSA with CS-PR, and for a problem $P$ from class $\P$,  we show $i)$ there exists a constant step-size $\alpha_P>0$ for which the mean-square error converges to zero at a rate of $O(\frac{C_P}{t})$, and in general the problem dependence (due to $P$ in $\alpha_P$ and $C_P$) cannot be avoided. We are also interested in the temporal difference class of LSA algorithms arising in reinforcement learning to solve the problem of approximate policy evaluation from experience replay. Here, we show that for the TD(0) algorithm universal step-size choice exists for $i)$ class of \emph{on-policy} problems with bounded features and $ii)$ the class of \emph{off-policy} problems with normalized features. We also introduce a modified version of GTD algorithm for which universal step-size choice exists for class of problems with bounded features. We also present numerical experiments that illustrate the theory.

\end{comment}