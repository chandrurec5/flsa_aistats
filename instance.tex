%!TEX root =  flsa.tex
\section{Instance Dependent Bounds}\label{sec:mainresults}
We first show that any  CALSA scheme satisfying \Cref{assmp:lsa} achieves an asymptotic rate of $O(\frac{1}{t})$ with a instance dependent step-size (see \Cref{th:rate}). In the \Cref{sec:land,sec:rl}, we investigate conditions under which this instance dependence can be eliminated such that we have \emph{universal} step-size choice and \emph{uniform} rates.
As a first step, we need show that there exists an instance dependent step-size such that the iterates are stable. To this end, it is useful to look at the error (of un-averaged iterates) $e_t\eqdef\theta_t-\ts$ given as below:
\begin{align}\label{eq:errrecmain}
e_t&={\prod_{s=1}^t (I-\alpha A_s) e_0}+\alpha{\sum_{i=1}^t\prod_{s=j+1}^t (I-\alpha A_s)\zeta_i}
\end{align}
It is clear that for the iterates to be stable a necessary condition is that the products of random matrices  of the form $\Pi_{s=t'}^{t}(I-\alpha A_s),\,t'=1,\ldots,t-1$ appearing in \eqref{eq:errrecmain} should not blow up. Through \Cref{def:spect} and \Cref{lm:hur} we show that (under \Cref{assmp:lsa}), for any given problem instance $P$, there exists a range of step-sizes for which this necessary condition can be satisfied.
\begin{definition}\label{def:spect}
Let $A_t$ be as in \Cref{assmp:lsa}, and $U\in\C^{\dcd}$ be any invertible matrix. Now define $\Lambda_t\eqdef U^{-1}A_t U$, $\Lambda\eqdef \EE{U^{-1}A U}$,
\begin{align*}
&\rhos{P_U}\eqdef \lmin{\Lambda+\Lambda^*-\alpha \EE{\Lambda_t^*\Lambda_t}}\,\\
& \rhod{P_U}\eqdef \lmin{(\Lambda+\Lambda^*)-\alpha \Lambda^* \Lambda},
\end{align*}
\end{definition}
where in $\rhod{P_U}$ and $\rhos{P_U}$, the subscripts $d$ and $s$ stands for \emph{deterministic} and \emph{stochastic} respectively. Also, note that $\rhos{P_U}<\rhod{P_U}$ holds from simple expectation algebra.
\begin{restatable}{lemma}{lmhur}\label{lm:hur}
Under \Cref{assmp:lsa} there exists an $\alpha_{P_U}>0$ and an invertible $U\in \C^{\dcd}$ such that $\rhod{P_U}>0$ and $\rhos{P_U}>0$ holds for all $\alpha \in (0,\alpha_{P_U})$. 
\end{restatable}
In \Cref{lm:hur}, $U$ is a change of basis matrix, and by defining a transformed iterate $\gamma_t\eqdef U^{-1}\theta_t$, we have $\gamma_t=(I-\alpha \Lambda_t) \gamma_t +\alpha U^{-1}b_t$. Then the stability of $\gamma_t$ implies stability of $\theta_t$; the former can be ensured if products $\Pi_{s=t'}^t (I-\alpha \Lambda_s),\,t'=1,\ldots,t-1$ don't blow up. Note that $\EE{\norm{I-\alpha \Lambda_t}^2}=1-\alpha \rhos{P_U}$, and thus \Cref{lm:hur} implies that $\EE{\norm{I-\alpha \Lambda_t}^2}<1,\,\forall \alpha\in (0,\alpha_{P_U})$. Here, $U$ transform helps to convert the given \emph{Hurwitz} matrix into a suitably scaled Jordan form, which is simpler to handle (see \Cref{sec:appendix} for more details).

Now, we present the result that shows that CALSA procedure achieves an instance dependent $O(\frac{1}{t})$ asymptotic rate.
\begin{restatable}{theorem}{thrate}\label{th:rate}
Let $P$ be a distribution over $\R^d\times \R^{\dcd}$ satisfying \Cref{assmp:lsa}.
Then, for  $U\in\C^{\dcd}$ and $\alpha_{P_U}>0$ as in \Cref{lm:hur},
\todoc{Why not use $\alpha_P$? $U$ is dependent on $P$. If $\alpha_{P_U}$ does not have a specific definition, there is no point to denote this dependence on $U$. It just adds to the clutter.}
for all $\alpha\in (0,\alpha_{P_U})$ and for all $t\ge 0$,
\begin{align*}
\EE{\normsm{\thh_t-\ts}^2}
\leq
\nu\,
\left\{\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}+ \frac{v^2}{t+1} \right\}\,,
\end{align*}
where $\nu = \left(1+\tfrac4{\alpha\rhod{P_U}}\right)\tfrac{\cond{U}^2}{\alpha \rhos{P_U}}$ and
$v^2 = 
\alpha^2(\sigma_{A_P}^2\norm{\ts}^2+\sigma_{b_P}^2)+\alpha (\sigma_{A_P}^2\norm{\ts})\norm{\theta_0-\ts}$.
\end{restatable}
Note that $\nu$ depends on $P_U$ and $\alpha$, while $v^2$ in addition also depends on $\theta_0$. The dependence,  when it is essential, will be shown as a subscript.


\begin{restatable}{theorem}{thlb}[Lower Bound]\label{th:lb}
There exists a distribution $P$ over $\R^d\times \R^{\dcd}$ satisfying \Cref{assmp:lsa}, such that
there exists $\alpha_P>0$ so that $\rhos{P}>0$ and $\rhod{P}>0$ hold for all $\alpha\in (0,\alpha_P)$ and
for any $t\ge 1$, \todoc{$t\ge 1$?} $\EE{\normsm{\thh_t-\ts}^2} 
\geq \frac{1}{\alpha^2 \, \rhod{P}\rhos{P}} \,\left\{ \frac{\beta_t \norm{\theta_0-\ts}^2}{(t+1)^2} 
+ \frac{v^2\sum_{s=1}^t \beta_{t-s}  }{(t+1)^2} \right\}\,,$
where $\beta_{t} =  \big(1-(1-\alpha \rhos{P})^t\big)$ and $v^2$ is as in \Cref{th:rate}.
\end{restatable}
Note that $\beta_t \to 1$ as $t\to\infty$. Hence, the lower bound essentially matches the upper bound.
%\subsection{Discussion}
In what follows, we discuss in brief the results in \Cref{th:rate,th:lb}. 
\textbf{Bias and Variance}: The MSE at time $t$ is bounded by a sum of two terms. The first \emph{bias} term is given by $\B=\nu \,\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}$, bounding how fast the initial error $\norm{\theta_0-\ts}^2$ is forgotten. The second \emph{variance} term is given by $\V=\nu\, \frac{v^2}{t+1} $ and captures the rate at which noise is rejected. 

\textbf{Behavior for extreme values of $\alpha$}: As $\alpha\to 0$, the bias term blows up, due to the presence of $\alpha^{-1}$ there. This is unavoidable (see also \cref{th:lb}) and is due to the slow forgetting of initial conditions for small $\alpha$. Small step-sizes are however useful to suppress noise, as seen from that in our bound $\alpha$ is seen to multiply the variances $\sigma^2_{A_P}$ and $\sigma^2_{b_P}$. In quantitative terms, we can see that the $\alpha^{-2}$ and $\alpha^2$ terms are trading off the two types of errors. For larger values of $\alpha$ with $\alpha_{P_U}$ chosen so that $\rhos{P_U}\ra 0$ as $\alpha\ra \alpha_{P_U}$ the bounds blow up again. 
Use of such similarity transformation have also been considered in analysis of RL algorithms \cite{lihong}. More generally, one can always take $U$ in the result that leads to the smallest bound.

\textbf{Proof Sketch:} We can expand the MSE as $\EE{\norm{\eh_t}^2}=\tfrac{1}{(t+1)^2}\, \ip{ \textstyle\sum_{s=0}^t e_s,\textstyle\sum_{s=0}^t e_s}\,,$
where $\eh_t = \thh_t-\ts$ and $e_t = \theta_t-\ts$, and the inner product is a summation of \emph{diagonal} terms $\EE{\ip{e_s,e_s}}$ and \emph{cross} terms of $\EE{\ip{e_s,e_q}}$, $s\neq q$. Since, we also use a basis transformation via $U$, the growth of the diagonal terms and the cross terms depends on the spectral norm of the random matrices $I-\alpha \Lambda_t$ and that of the deterministic matrix $I-\alpha \Lambda$, respectively. This explains the reason as to why $\rhos{P_U}$ and $\rhod{P_U} $ appear in the bounds.

\textbf{The lower bound} of \Cref{th:lb} shows that the upper bound of \Cref{th:rate} is tight in a number of ways.
In particular, the coefficients of both the $1/t$ and $1/t^2$ terms inside $\{ \cdot \}$ are essentially matched (here $U=I$).
Further, we also see that the $(\rhos{P_U}\rhod{P_U})^{-1}$ appearing in $\nu = \nu_{P_U,\alpha}$ cannot be removed from the upper bound. 
Note however that there are specific examples, such as SGD for linear least-squares,
where this latter factor can in fact be avoided (for further remarks see \Cref{sec:related}).

%\textbf{Goal:} In \Cref{th:rate} we have established the fact that under \Cref{assmp:lsa} CALSA achieves an \emph{asymptotic} rate of $O(\frac{1}{t})$ for the MSE with an instance dependent step-size range of $(0,\alpha_{P_U})$, and the  constant $\nu$ has instance dependent terms. In the \Cref{sec:land,sec:rl}, we investigate conditions under which this instance dependence can be eliminated such that we have \emph{universal} step-size choice and \emph{uniform} rates.

