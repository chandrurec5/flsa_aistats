%!TEX root =  flsa.tex
\section{Instance Dependent Bounds}\label{sec:mainresults}
Let $\hat e_t = \hat \theta_t - \ts$ be the error of estimating $\ts$.
In this section we consider instance dependent bounds on the expected squared parameter estimation error $\E[ \norm{\hat e_t}^2 ]$. 
The consideration of weighted norms is postponed until the next section;
our interest in the instance dependent analysis stems from that it 
provides the foundation for the analysis coming in the next sections. 
While the results presented here may not have appeared in exactly the form presented here in the literature,
the analysis follows standard ideas. \todoc{Add bunch of relevant references..}
\iffalse
The core question is how the error depends on various instance-dependent quantities (variance of noise, spectral properties of matrices involved, the size of the initial error, etc.), how fast the error decreased over time.
As we shall see, if the constant step-size belongs to a small enough neighborhood of zero (the size of which depends on the instance), the error decreases at a rate of $O(1/t)$ and both too small and too large step-sizes are undesirable. The main result, \Cref{th:rate}, which bounds the error from above, is corroborated with a lower bound which shows that the upper bound is tight.
While the results of this section may not have appeared in this form in the literature, the steps of the analysis are quite standard. \todoc{I think we should say who we are building on.}
\fi

\iffalse
We first show that any  CALSA scheme  achieves an error of size$O(\frac{1}{t})$ with instance dependent constants and
step-size (see \Cref{th:rate}). In the \Cref{sec:land,sec:rl}, we investigate conditions under which this instance dependence can be eliminated such that we have \emph{universal} step-size choice and \emph{uniform} rates.
As a first step, we need show that there exists an instance dependent step-size such that the iterates are stable. To this end, it is useful to look at the error (of un-averaged iterates) $e_t\eqdef\theta_t-\ts$ given as below:
\fi
Fix an instance $P$. Thus, $(A_t,b_t)\sim P$. To minimize clutter, dropping the subindex $P$ from $A_P$ and $b_P$, we let $A = \EE{A_t}$ and $b = \EE{b_t}$.
Straightforward calculation gives that $\hat e_t = \frac1{t+1} \sum_{s=0}^t e_s$, where
 $e_s\eqdef\theta_s-\ts$ is the error of the $s$th un-averaged iterate. Further calculation shows that \begin{align}\label{eq:errrecmain}
e_t&={\prod_{s=1}^t (I-\alpha A_s) e_0}+\alpha{\sum_{i=1}^t\prod_{s=j+1}^t (I-\alpha A_s)\zeta_i}\,,
\end{align}
where $\zeta_i = b_i-b - (A_i -A)\ts$ is the ``noise'' component in $(A_i,b_i)$.

From \eqref{eq:errrecmain}, it is clear that the magnitude of $\hat e_t$ is governed by the behavior
of the products $\Pi_{s=j+1}^{t}(I-\alpha A_s)$, $j=0,\dots,t$. 
Here, we will perform some crude upper bounding and derive simple sufficient conditions that guarantee that on expectation these product matrices are ``small''. However, we will show that the error bounds derived are tight in a worst-case sense.

To motivate the next results, note that repeated conditioning shows that
$\phi \defeq \norm{ \EE{ (I-\alpha A_1)^\top (I-\alpha A_1)} }<1$ is sufficient 
 to guarantee that the expectation of the squared norm of the
 first term in \eqref{eq:errrecmain} vanishes with growing $t$.
Here, we can use $A_1$, because $(A_t)_t$ is i.i.d.
Now, thanks to
$\EE{ (I-\alpha A_1)^\top (I-\alpha A_1)} = 
I - \alpha \left\{(A+A^\top) -\alpha \EE{A_1^\top A_1}\right\}$,
we see that the eigenvalues of this matrix are of the form $1-\alpha \lambda$,
where $\lambda$ is an eigenvalue of the symmetric matrix 
$S = (A+A^\top) -\alpha \EE{A_1^\top A_1}$.
Hence, if all eigenvalues of $S$ lie in the interval $(0,2/\alpha)$, we have
$\phi<1$. Now since $\EE{A_1^\top A_1}$ is a positive semidefinite matrix,
$\lambda_{\max}(S) \le \lambda_{\max}(A+A^\top)-\alpha \lambda_{\min}(\EE{A_1^\top A_1})\le \lambda_{\max}(A+A^\top)$, this $\lambda_{\max}(S)<2/\alpha$ holds if 
$\lambda_{\max}(A+A^\top)<2/\alpha$.
On the other hand, $\lambda_{\min}(S)>0$ holds
provided
\begin{align*}
\rho_s(\alpha,P) \defeq \lambda_{\min} ((A+A^\top) -\alpha \EE{A_1^\top A_1} )>0.
\end{align*}
Now, $\rho_s(\alpha,P) \ge \lambda_{\min}( A+A^\top ) - \alpha \lambda_{\max} (\EE{ A_1^\top A_1})$. Thus, if $\lambda_{\min}(A+A^\top)>0$, for $\alpha>0$ small enough, it follows that $\rho_s(\alpha,P)>0$. Note that there are Hurwitz matrices $A$ such that $\lambda_{\min}(A+A^\top)<0$. However, one can show  that every Hurwitz matrix is similar to a real matrix $B$ such that $B+B^\top$ is SPD (for details, see \cref{sec:hurpd}).


It is clear that for the iterates to be stable a necessary condition is that the products of random matrices  of the form $\Pi_{s=t'}^{t}(I-\alpha A_s),\,t'=1,\ldots,t-1$ appearing in \eqref{eq:errrecmain} should not blow up. Through \Cref{def:spect} and \Cref{lm:hur} we show that (under \Cref{assmp:lsa}), for any given problem instance $P$, there exists a range of step-sizes for which this necessary condition can be satisfied.
\begin{definition}\label{def:spect}
Let $A_t$ be as in \Cref{assmp:lsa}, and $U\in\C^{\dcd}$ be any invertible matrix. Now define $\Lambda_t\eqdef U^{-1}A_t U$, $\Lambda\eqdef \EE{U^{-1}A U}$,
\begin{align*}
&\rhos{P_U}\eqdef \lmin{\Lambda+\Lambda^*-\alpha \EE{\Lambda_t^*\Lambda_t}}\,\\
& \rhod{P_U}\eqdef \lmin{(\Lambda+\Lambda^*)-\alpha \Lambda^* \Lambda},
\end{align*}
\end{definition}
where in $\rhod{P_U}$ and $\rhos{P_U}$, the subscripts $d$ and $s$ stands for \emph{deterministic} and \emph{stochastic} respectively. Also, note that $\rhos{P_U}<\rhod{P_U}$ holds from simple expectation algebra.
\begin{restatable}{lemma}{lmhur}\label{lm:hur}
Under \Cref{assmp:lsa} there exists an $\alpha_{P_U}>0$ and an invertible $U\in \C^{\dcd}$ such that $\rhod{P_U}>0$ and $\rhos{P_U}>0$ holds for all $\alpha \in (0,\alpha_{P_U})$. 
\end{restatable}
In \Cref{lm:hur}, $U$ is a change of basis matrix, and by defining a transformed iterate $\gamma_t\eqdef U^{-1}\theta_t$, we have $\gamma_t=(I-\alpha \Lambda_t) \gamma_t +\alpha U^{-1}b_t$. Then the stability of $\gamma_t$ implies stability of $\theta_t$; the former can be ensured if products $\Pi_{s=t'}^t (I-\alpha \Lambda_s),\,t'=1,\ldots,t-1$ don't blow up. Note that $\EE{\norm{I-\alpha \Lambda_t}^2}=1-\alpha \rhos{P_U}$, and thus \Cref{lm:hur} implies that $\EE{\norm{I-\alpha \Lambda_t}^2}<1,\,\forall \alpha\in (0,\alpha_{P_U})$. Here, $U$ transform helps to convert the given \emph{Hurwitz} matrix into a suitably scaled Jordan form, which is simpler to handle (see \Cref{sec:appendix} for more details).

Now, we present the result that shows that CALSA procedure achieves an instance dependent $O(\frac{1}{t})$ asymptotic rate.
\begin{restatable}{theorem}{thrate}\label{th:rate}
Let $P$ be a distribution over $\R^d\times \R^{\dcd}$ satisfying \Cref{assmp:lsa}.
Then, for  $U\in\C^{\dcd}$ and $\alpha_{P_U}>0$ as in \Cref{lm:hur},
\todoc{Why not use $\alpha_P$? $U$ is dependent on $P$. If $\alpha_{P_U}$ does not have a specific definition, there is no point to denote this dependence on $U$. It just adds to the clutter.}
for all $\alpha\in (0,\alpha_{P_U})$ and for all $t\ge 0$,
\begin{align*}
\EE{\normsm{\thh_t-\ts}^2}
\leq
\nu\,
\left\{\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}+ \frac{v^2}{t+1} \right\}\,,
\end{align*}
where $\nu = \left(1+\tfrac4{\alpha\rhod{P_U}}\right)\tfrac{\cond{U}^2}{\alpha \rhos{P_U}}$ and
$v^2 = 
\alpha^2(\sigma_{A_P}^2\norm{\ts}^2+\sigma_{b_P}^2)+\alpha (\sigma_{A_P}^2\norm{\ts})\norm{\theta_0-\ts}$.
\end{restatable}
Note that $\nu$ depends on $P_U$ and $\alpha$, while $v^2$ in addition also depends on $\theta_0$. The dependence,  when it is essential, will be shown as a subscript.


\begin{restatable}{theorem}{thlb}[Lower Bound]\label{th:lb}
There exists a distribution $P$ over $\R^d\times \R^{\dcd}$ satisfying \Cref{assmp:lsa}, such that
there exists $\alpha_P>0$ so that $\rhos{P}>0$ and $\rhod{P}>0$ hold for all $\alpha\in (0,\alpha_P)$ and
for any $t\ge 1$, \todoc{$t\ge 1$?} $\EE{\normsm{\thh_t-\ts}^2} 
\geq \frac{1}{\alpha^2 \, \rhod{P}\rhos{P}} \,\left\{ \frac{\beta_t \norm{\theta_0-\ts}^2}{(t+1)^2} 
+ \frac{v^2\sum_{s=1}^t \beta_{t-s}  }{(t+1)^2} \right\}\,,$
where $\beta_{t} =  \big(1-(1-\alpha \rhos{P})^t\big)$ and $v^2$ is as in \Cref{th:rate}.
\end{restatable}
Note that $\beta_t \to 1$ as $t\to\infty$. Hence, the lower bound essentially matches the upper bound.
%\subsection{Discussion}
In what follows, we discuss in brief the results in \Cref{th:rate,th:lb}. 
\textbf{Bias and Variance}: The MSE at time $t$ is bounded by a sum of two terms. The first \emph{bias} term is given by $\B=\nu \,\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}$, bounding how fast the initial error $\norm{\theta_0-\ts}^2$ is forgotten. The second \emph{variance} term is given by $\V=\nu\, \frac{v^2}{t+1} $ and captures the rate at which noise is rejected. 

\textbf{Behavior for extreme values of $\alpha$}: As $\alpha\to 0$, the bias term blows up, due to the presence of $\alpha^{-1}$ there. This is unavoidable (see also \cref{th:lb}) and is due to the slow forgetting of initial conditions for small $\alpha$. Small step-sizes are however useful to suppress noise, as seen from that in our bound $\alpha$ is seen to multiply the variances $\sigma^2_{A_P}$ and $\sigma^2_{b_P}$. In quantitative terms, we can see that the $\alpha^{-2}$ and $\alpha^2$ terms are trading off the two types of errors. For larger values of $\alpha$ with $\alpha_{P_U}$ chosen so that $\rhos{P_U}\ra 0$ as $\alpha\ra \alpha_{P_U}$ the bounds blow up again. 
Use of such similarity transformation have also been considered in analysis of RL algorithms \cite{lihong}. More generally, one can always take $U$ in the result that leads to the smallest bound.

\textbf{Proof Sketch:} We can expand the MSE as $\EE{\norm{\eh_t}^2}=\tfrac{1}{(t+1)^2}\, \ip{ \textstyle\sum_{s=0}^t e_s,\textstyle\sum_{s=0}^t e_s}\,,$
where $\eh_t = \thh_t-\ts$ and $e_t = \theta_t-\ts$, and the inner product is a summation of \emph{diagonal} terms $\EE{\ip{e_s,e_s}}$ and \emph{cross} terms of $\EE{\ip{e_s,e_q}}$, $s\neq q$. Since, we also use a basis transformation via $U$, the growth of the diagonal terms and the cross terms depends on the spectral norm of the random matrices $I-\alpha \Lambda_t$ and that of the deterministic matrix $I-\alpha \Lambda$, respectively. This explains the reason as to why $\rhos{P_U}$ and $\rhod{P_U} $ appear in the bounds.

\textbf{The lower bound} of \Cref{th:lb} shows that the upper bound of \Cref{th:rate} is tight in a number of ways.
In particular, the coefficients of both the $1/t$ and $1/t^2$ terms inside $\{ \cdot \}$ are essentially matched (here $U=I$).
Further, we also see that the $(\rhos{P_U}\rhod{P_U})^{-1}$ appearing in $\nu = \nu_{P_U,\alpha}$ cannot be removed from the upper bound. 
Note however that there are specific examples, such as SGD for linear least-squares,
where this latter factor can in fact be avoided (for further remarks see \Cref{sec:related}).

%\textbf{Goal:} In \Cref{th:rate} we have established the fact that under \Cref{assmp:lsa} CALSA achieves an \emph{asymptotic} rate of $O(\frac{1}{t})$ for the MSE with an instance dependent step-size range of $(0,\alpha_{P_U})$, and the  constant $\nu$ has instance dependent terms. In the \Cref{sec:land,sec:rl}, we investigate conditions under which this instance dependence can be eliminated such that we have \emph{universal} step-size choice and \emph{uniform} rates.

