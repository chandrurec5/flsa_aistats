%!TEX root =  flsa.tex
\section{Universal stepsizes in LVE}\label{sec:rl}
We now turn to the question of the existence of universal stepsizes for CATD(0) and CAGTD (cf. \cref{tb:tdalgos}). \todoc{Extensions to TD($\lambda$) etc.?}
In what follows, we define what we call \emph{admissibility}, 
a sufficient condition for the existence of a universal stepsize. 
\begin{definition}\label{def:admis}
Call a problem class $\P$ \emph{admissible} if there exists a unique $U$ and $\alpha_{\P_U}>0$ such that
$\rhos{P_U}>0$ holds for all $P\in \P$ and $\alpha\in(0,\alpha_{\P_U})$.
\end{definition}
If $\P$ is admissible, it  follows from \Cref{th:rate} that an asymptotic ``fast'' rate of $O(\frac1t)$ is achieved for any $P\in \cP$. 
We now define three LVE problem classes. For the definitions introduce
the entrywise $\max$-norm for matrices: $\norm{A}_{\max} = \max_{i,j} |A_{ij}|$.
Recall that an LVE problem is given by the joint distribution of the i.i.d. sequence
$\{(\phi_t,\phi_t',r_t)\}_{t\ge 1}\subset \R^d\times \R^d \times \R$. 
The classes we define are as follows:
%satisfy $\norm{A_t}_{\max}\le B_{GTD}$ and $\norm{\phi_t}_{\max}\le B_{TD}$ with known constants $B_{TD}\ge 1$ and $B_{GTD}\ge 1$, \todoc{I assume $B\ge 1$ for simplicity so that $B^4\ge B^2$ holds.}
%as well as the following conditions:
\begin{align*}
\text{SOFS}(B)&\!\!: \E[\phi_t\phi_t^\top]=\E[\phi'_t(\phi'_t)^\top], % \\
						%& \,\,\,\,\, 
						\norm{\phi_t}\le B\,, t\ge 1\,.\\
\text{GTDOFF}(B)&\!\!:  \norm{A_t}_{\max} \le B, t\ge 1\,.
\end{align*}
Here, $\text{SOFS}$ stands for \emph{second-order feature stationarity}, 
while $\text{GTDOFF}$ stands for ``off-policy'', a specific nomenclature borrow from the RL literature.
%(The second-order feature stationarity constraint replaces the ``on-policy'' constraint that is often used to guarantee stability of the TD algorithm.)
Note that the ``second order feature stationarity'' condition $\E[\phi_t\phi_t^\top]=\E[\phi'_t(\phi'_t)^\top]$ will hold 
when sampling (in the underlying Markov reward process) 
is started from the stationary distribution. 
Note that the constants $B$ appearing in the two classes constraint the data in different ways.
\if0
\FloatBarrier
\begin{table}[h]
\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|c|c|c|}\hline
Class  &Variable &Remark\\\hline
$\P_{TDON}$  & $\pi,P,R$ &$\mu=d_{\pi}, \EE{\phi_t\phi_t}=\EE{\phi'_t\phi'_t}$\\\hline
$\P_{TDOFF}$  & $\pi,\mu,P,R$ &$\mu\neq d_{\pi}, \EE{\phi_t\phi_t}=\EE{\phi'_t\phi'_t}$, $\norm{\phi_t}=1$\\\hline
$\P_{GTDOFF}$ & $\pi,\mu,P,R$ &$\mu\neq d_{\pi}$\\\hline
\end{tabular}
}
\caption{Here, $S,A,\gamma$ are fixed across all the class and the second column shows the quantities that vary across the respective classes. These three capture \emph{on/off-policy} learning scenarios arising in RL.}
\end{table}
\fi
\begin{restatable}{theorem}{thtdadmis}\label{th:tdadmis}
Let $B\ge 1$.
The following hold:
%Define constant $B\eqdef\max_{ij}\md{A_{ij}}$.  We have 
$i)$ CATD(0) has a universal stepsize of $\alpha_{td}=\frac{1}{B}$ for  the class $\text{SOFS}(B)$.
$ii)$ CAGTD has a universal stepsize of $\alpha_{gtd}=\frac{1}{2B^2 d}$ for the class $\text{GTDOFF}(B)$.
\end{restatable}
In the proof we show that the three classes are admissible for the respective algorithms.
\if0
The property $\EE{\phi_t\phi_t}=\EE{\phi'_t{\phi'_t}^\top}$ is known as second order feature stationarity, and is used in the proof.
\fi

From \Cref{tb:tdalgos}, the matrix $A_t=\phi_t(\phi_t-\gamma\phi'_t)^\top$ is key to both CATD(0) and CAGTD. In the case of CATD(0),  the expression for $\rhos{P}$ involves $\phi^\top_t\phi_t$ which can take a maximum value of $B^2d$  and is the reason for the $B^2d$ term in $\alpha_{td}$. In the case of CAGTD, the expression for $\rhos{P}$ involves $A_t^\top A_t$, and hence $B^2_{GTD}d$ appears in $\alpha_{gtd}$. 
The conservative stepsize for CAGTD seems to be the price paid for \emph{off-policy} stability. Notice that in the case of normalized features, the factor depending on $B$ and $d$ can be removed.

Note that the above result in particular implies that the respective algorithms with the proposed stepsizes achieve the instance-dependent errors $O(\frac{1}{t})$ on these three classes of LVE problems.

