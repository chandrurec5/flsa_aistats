\section{Problem Landscape}
In this section we try to understand following are possible questions:
\begin{enumerate}[leftmargin=*] 
\item (Q1) Is is possible to obtain uniformly fast asymptotic rates, i.e., given a problem class $\P$ does the MSE converge to zero at a rate $O(\frac{1}{t})$\footnote{$\frac{1}{t}$ is the statistical rate.} as $t\ra\infty$?
\item (Q2) Is it possible to obtain uniform fast rates in finite-time, i.e., given a problem class $\P$ we want the MSE converges to zero at a rate $\frac{C}{t}$, where the $C$ is independent of $\P$?
\end{enumerate}
It turns out that the answers to both Q1 and Q2 is \emph{negative}, and we present (informal) arguments for the same.

\paragraph{Q2- Uniformly fast finite-time rate:} Consider the following special case of CALSA with additive noise:
\begin{subequations}\label{eq:lsaadd}
\begin{align}
\label{conststep}&\text{LSA:} &\theta_t&=\theta_{t-1}+\alpha(b_t-A_P\theta_{t-1})\,,\\
\label{iteravg}&\text{PR-Average:} &\thh_t&=\frac{1}{t+1}{\sum}_{s=0}^{t}\,\theta_i\,.
\end{align}
\end{subequations}
Notice that \eqref{eq:lsaadd} is a special case of \eqref{eq:lsa} with $A_t=A_P,\forall t\geq 0$. For the \emph{error} variable defined as $\eh_t\eqdef\thh_t-\ts$, the following proposition holds.
\begin{proposition}\label{prop:erradd}
For all $\alpha_P>0$ such that $\Lambda(I-\alpha_P A)<1$, we have
\begin{align*}
\eh_t&=\frac{1}{t+1}(\alpha_P A)^{-1}\big([I-(I-\alpha_P A)^{t+1}e_0\\
&+\alpha_P\sum_{s=1}^t [I-(I-\alpha_P A)^{t+1-s}] N_s]\big)
\end{align*}
\end{proposition}
In what follows, we would like to understand what CALSA holds for the problem class $\P_{SPD}$ where $A_P$ is real symmetric and positive definite with $\Lambda(A_P)<1$ for all $P\in P_{SPD}$. In particular, we want to know for a fixed $t$ and adversarial choice of problem instance from $\P_{SPD}$ whether it is possible to choose a constant step-size $\alpha>0$ so that we get desirable rates of convergence for the MSE.  
We state our observation in the following remarks:
\begin{itemize}[leftmargin=*]
\item \textbf{Step-Size}: We can choose any $\alpha\in(0,1)$ and it follows that $\Lambda(I-\alpha A)<1, \forall P\in \P_{SPD}$. Thus, there exists a universal step-size choice for $\P_{SPD}$.
\item \textbf{Estimation Case:} For small $t$, i.e., $t$ such that $\Lambda(\alpha t A_P)<1$ we have $\E{\norm{\eh_t}^2}\approx \frac{1}{(t+1)^2}(\B (t+1)^2+ \alpha^2\sigma_b^2 O(t^3))=\B+\alpha^2\sigma_b^2 O(t)$. Thus for a fixed $t>0$ and bias $\B$, it follows that no matter how we choose $\alpha$, we would suffer a $O(\B)$ error.
\item \textbf{Prediction Case:} For small $t$, i.e., $t$ such that $\Lambda(\alpha t A_P)<1$ we have $\E{\norm{\eh_t}^2_A}\approx O(\frac{\B}{\alpha t})+O(\sigma^2_b\alpha)$, where we can balance the bias and the variance terms by choosing $\alpha=\frac{1}{\sqrt{t}}$.
\item \textbf{Scaling Noise Case:} For small $t$, i.e., $t$ such that $\Lambda(\alpha t A_P)<1$, and $\sigma_b^2\leq \Lambda(A_P)$ we have $\E{\norm{\eh_t}^2_A}\approx O(\frac{\B}{\alpha t})+O(\frac{1}{t})$, and by choosing $\alpha=1$ we achieve a rate of $O(\frac{1}{t})$.
\end{itemize}
Thus, it turns out that a uniformly fast finite-time rate of $O(\frac{1}{t})$ can be achieved only under special structure on the noise and we way in which the error is measured. For instance, we have pressing evidence to suspect that in the additive noise case, the linear least squares problem achieves uniform rate of $O(\frac{1}{t})$. We believe the question in the multiplicative noise is still open (for more see \Cref{sec:rel}).
\paragraph{Q1 Uniformly fast asymptotic rate:} A necessary condition to achieve an uniform asymptotic rate is to be choose a step-size $\alpha>0$ that is problem independent. 
We show a problem class $\P$ where such an instance independent choice is not possible: consider the deterministic class $\P_{det}$ where $b_t=\mathbf{0}$, $A_t=A_P, \forall t\geq 0$ and $A_P=\left\{\left[\begin{matrix} u &v \\ -v & u\end{matrix}\right] : \right\}$ and the class $\P_{det}$ is generated by the set $\{(u,v) : u^2+v^2\leq B\}$. It is clear that the data from $\P_{det}$ is bounded. Is it also true that there exists a universal step-size for $\P_{det}$? The answer is no:  
\begin{proposition}\label{prop:unistep}
There does not exist $\alpha$ such that $\Lambda(I-\alpha A_P)<1, \forall P\in \P$. 
\end{proposition}

\paragraph{A sufficient Condition:} Going by the negative answers for Q1 and Q2, we shift our focus to obtaining uniform asymptotic rates. However, in order to do so we need a verifiable condition that implies existence of a constant step-size across a given problem class $\P$. To this end, we introduce the notion of what we call \emph{admissibility}.
\begin{definition}\label{def:admis}
Call a set of distributions $\P$ over $\C^{d}\times \C^{\dcd}$
\emph{admissible} if there exists $\alpha_{\P}>0$ such that
$\rhos{P}>0$ holds for all $P\in \P$ and $\alpha\in(0,\alpha_{\P})$.
\end{definition}
It is easy to see that $\alpha \mapsto \rhos{P}$ is decreasing,
hence if $\alpha_{\P}>0$ witnesses that $\P$ is admissible
then any $0<\alpha'\le \alpha_{\P}$ is also witnessing this. In simple terms, the class $\P$ admits a universal step-size choice. 

While the usefulness of \Cref{def:admis} is not immediately clear, we promise that in \Cref{sec:td} its usefulness can be seen in the context of problems in RL. 