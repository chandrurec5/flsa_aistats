%!TEX root =  flsa.tex
\textbf{Conclusion:} 
Step-size choice is critical in LSA algorithms, and especially in the case of TD algorithms, step-sizes are often treated as hyper-parameters that need to be tuned in a problem instance specific manner. To avoid this tuning, it is desirable to choose a single \emph{universal} step-size rule that works for all the instances in a problem class. This paper investigated the promise of an approach called CALSA (constant step-size averaged - LSA), an idea that goes back to \citet{ruppert} and \citet{polyak-judisky}. For a given problem class, we asked $i)$ whether a \emph{universal} constant step-size can be chosen and $ii)$ whether a \emph{uniform} rate of convergence for the MSE can be achieved, across the class. We showed that answers to these questions in general is \emph{no}. However, we showed (under our assumptions) that any CALSA achieves convergence rate of $O(\frac{C_P}{t})$ for MSE, where the constant $C_P>0$ is problem instance dependent. We then showed that TD algorithms with a problem independent universal constant step-size and iterate averaging, achieve an asymptotic fast rate of $O(\frac{1}{t})$. 
\subsubsection*{Acknowledgements}
Part of the work was done at the University of Alberta, and the authors greatly acknowledge the
support of NSERC and the Alberta Innovates Technology
Futures through the Alberta Machine Intelligence
Institute (AMII).