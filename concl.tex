%!TEX root =  flsa.tex
\textbf{Conclusion:} 
Stepsize choice is critical in LSA algorithms, and especially in the case of TD algorithms.
Stepsizes are often treated as hyper-parameters that need to be tuned in a problem instance specific manner. To avoid this tuning, it is desirable to choose a single \emph{universal} stepsize rule that works for all the instances in a problem class. This paper investigated the promise of an approach called CALSA (constant stepsize averaged linear stochastic approximation), based on an idea that goes back to \citet{ruppert} and \citet{polyak-judisky}. For a given problem class, we asked $i)$ whether a \emph{universal} constant stepsize can be chosen and $ii)$ whether a \emph{uniform} rate of convergence for the MSE can be achieved, across the class. We showed that answers to these questions in general is \emph{no}. However, we showed (under our assumptions) that any CALSA achieves convergence rate of $O(\frac{C_P}{t})$ for MSE, where the constant $C_P>0$ is problem instance dependent. We then showed that TD algorithms with a problem independent universal constant stepsize and iterate averaging, achieve a problem-dependent error 
that decays as $O(\frac{1}{t})$ with the number of iterations $t$.
\subsubsection*{Acknowledgements}
Part of the work was done at the University of Alberta, and the authors greatly acknowledge the
support of NSERC and the Alberta Innovates Technology
Futures through the Alberta Machine Intelligence
Institute (AMII).