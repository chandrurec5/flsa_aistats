%!TEX root =  flsa.tex
\section{Conclusion}
We presented a finite time performance analysis of LSAs with CS-PR and showed that the MSE decays at a rate $O(\frac{1}{t})$. Our results extended the analysis of \citet{bach} for SGD with CS-PR for the problem of linear least-squares estimation and \iid sampling to general LSAs with CS-PR. Due to the lack of special structures, our analysis for the case of general LSA cannot recover entirely the results of \citet{bach} who use the problem specific structures in their analysis.
Our results also improved the rates in the case of the GTD class of algorithms. We presented conditions under which a constant step-size can be chosen uniformly for a given class of data distributions. We showed a negative result in that not all data distributions `admit' such a constant step-size. This is a negative result from the perspective of TD algorithms in RL. We also argued that a problem dependent constant step-size can be obtained in an automatic manner and presented numerical experiments on a synthetic LSA. %Thus, LSAs with CS-PR technique $i)$ achieves comparable or better error rates $ii)$ alongside automatic tuning of the constant step-size can perform comparably well in practice.
